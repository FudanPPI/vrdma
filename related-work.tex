\section{Related Work}
This section introduces related works about network virtualizations, including traditional network and RDMA.

\subsection{Traditional Network Virtualization}
NIC(Virtual network cards) and virtual network bridges is important to traditional network virtualization. When two virtual instances communicate across servers, the traffic of vNIC will be forwarded to the physical network card through the virtual network bridge, and then through the remote physical network card and virtual network bridge to reach the destination virtual instance.


\subsection{RDMA Virtualization}
The solutions of RDMA virtualization including hardware-based  and software-based:

The basic solution is SR-IOV in hardware-based virtualization of RDMA. Its virtual layer is located in the hardware, so it is high-performance close to native RDMA. However, SR-IOV utilzie the limited hardware resouces to virtualize multiple VFs for different VMs or containers and that makes SR-IOV unscabale. Finnally, SR-IOV' virtual interfaces are located in hardware, so it lacks portability and other manageability without software virtual layer. 

In software virtualization, existing solutions treat virtual machines and containers differently. For containers, FreeFlow's target is to build a virtual switch for RDMA network~\cite{kim2019freeflow}. So, containers forwards all RDMA commands to the virtual layer to deal with, and that is ineffective. For virtual machines, HyV~\cite{pfefferle2015hybrid} ~\cite{pfefferle2014vverbs} separates the control path and data path for performance, it forwards control command to hypervisor and avoids forwarding data commands by mapping RDMA resources. However, hyv lacks the management of virtual RDMA networks. Although MasQ~\cite{he2020masq} makes up for this problem by network virtualization, its virtual layer is located in kernel space. Extending MasQ to the container environment will lose lightweight  management in user space for containers.




