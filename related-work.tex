\section{Related Work} \label{relatedwork}

In this section, we discuss the related works from the virtualization for traditional TCP/IP, hardware virtualization for RDMA and software virtualizaiotn for RDMA.

\textbf{Virtualization for traditional TCP/IP:}\quad TCP/IP is the basic protocol for each hardware node in data centers. Several software solutions were proposed for traditional network virtualization. For both VMs and containers, virtual network interface cards (vRNICs) are one solution, e.g., virtio-net~\cite{virtio-russell2008}, vhost-net~\cite{vhost-net},  vhost-user-net~\cite{vhost-user-net} for VMs and veth~\cite{veth} for containers. These vRNICs are implemented in VM's or container's different space~(i.e., kernel space or user space). Software-based switch, also knowed as virtual switch, bridges the vNIC to physical NIC in the local host. Virtual switch can run in user space~(e.g., Snabb Switch~\cite{snabb}), kernel space~(e.g., Linux bridge~\cite{linux-bridge}), or both~(e.g., Open vSwitch~\cite{ovs-2015}). 

\textbf{Hardware virtualization for RDMA:}\quad SR-IOV~\cite{sr-iov} splits a physical PCI-e device to multiple PCI-e virtual devices, which can be allocated to VMs, providing near-native performance. In comparison to software-based virtualization, it lacks of efficient resource management and suffers from the flexibility issue because that is implemented in hardware. In uniRDMA, the isolation of SR-IOV are utilized and the unscalable problems are solved by dynamic vRNIC mapping. 

\textbf{Software virtualization for RDMA:}\qquad FreeFlow~\cite{kim2019freeflow} forwards all RDMA commands from containers to the FreeFlow router~(FFR) including data path. It forwards the commands from applications to NIC drivers, such as creating QP/CQ and sending/receiving data. The key different between FreeFlow and \sys is that FreeFlow captures data path commands and forwards them to RDMA device, which involves additional overhead for memory copy. In contrast, \sys ditectly map the RDMA device memory to the guest users. Thus, a guest application can operate them without addtional memory copy. HyV~\cite{pfefferle2015hybrid} and MasQ~\cite{he2020masq} are designed for VMs. They achieve zero-copy and bypass the OS kernel on the data path. HyV/MasQ backend is implemented in kernel space while \sys vRNIC backend is implemented in user space for less impact on OS kernel and better use of the Verbs abstraction layer. Moreover, these RDMA virtualization techniques do not support both VMs and containers in the same framework.
