\section{Introduction}
RDMA (remote direct memory access) is a high-performance network,  with high throughput, low latency and low CPU load. Thus, RDMA is widely adopted in supercomputing for various applications,  such as artificial intelligence, and big data processing. 

Recently, migrating to cloud is a trend for supercomputing users. Different from traditional HPC environment, resource provisioning are done in the form of virtual machines or containers in clouds~\cite{hpc-cloud}. Thus, to use RDMA for applications in clouds, RDMA devices need to be virtualzed. Existing solutions for RDMA virtualization include software-based and hardware-based solutions. The software-based solutions are mainly about HyV, FreeFlow or MasQ~\cite{pfefferle2015hybrid}~\cite{kim2019freeflow}~\cite{he2020masq}. FreeFlow is  designed for containers and HyV/MasQ are proposed for virtual machines. The hardware-based solution is based on PCI pass-through for less overhead, like SR-IOV~\cite{sr-iov}. 

However, in practice, hybrid virtual environments are common for clouds. In specific, VMs, containers orother form of virtualization (e.g. containers in VMs) can be found in the same datacenter or even on the same host machine. The existing solutions are not suitable for hybrid virtual environments for the following reasons:

For software-based solutions, they are designed for specific virtual environments. Thus, in hybrid virtual environments, we should deploy multiple frameworks or extend one framework for another environment. If multiple frameworks co-exist in the same cluster, RDMA resources should be divided statically to avoid management conflict. This may cause the low resource utilization and higher management complexity. If single framework is extended to hybrid virtual environments: for FreeFlow, the communication between applications and the virtual layer does not suit VMs and it has apperant overhead in RDMA network without bypassing the virtual layer; for HyV or MasQ, compared to FreeFlow, their virtual layers are in kernel-space that brings new problems: the kernel's attack surface is larger due to lots of inserted code in kernel, managment development is inflexible in kernel programming, and the inserted modules are depenedent on hardware-specific RDMA kernel drivers.

For hardware-based solutions (e.g. SR-IOV), RDMA device resources are directly allocated to virtual machines or containers. Thus, the device utilization is static and inefficient. Moreover, RDMA networks in clouds are still managed by physical switches or routers. Thus, virtual network management is not scalable and portable in large-scale clouds.

To address these problems, we propose a unified RDMA virtualization framework for hybrid virtual environment in clouds, namely uniRDMA, also with the design goals of flexible management and high-performance. 

As the basic abstract unit of uniRDMA, each vRNIC (virtual RDMA network interface card) is virtualized in user-space with basic RDMA attributes, such as QPs (Queue Pairs). Thus, vRNICs are flexible and hardware-awareness. To support VMs or containers unifiedly, vRNICs are the device of VMs and containers and a specific kernel driver is in guest OS. VMs and containers use the same communication protocal to vRNICs. Moreover, to optimize the performance, RDMA resources in vRNICs are mapped to applications in VMs or containers. 

Virtual layer tacks charge of vRNICs in each host server, including vRNICs instantiation and vRNICs mapping to physical RNIC. Besides, the virtual layer is also a software virtual switch with configuring vRNIC address and routing rules. 

Finally, we implement the prototype and evaluate it in different aspectsbenchmarks, such as throughput, latency, scalability, with and real-word applications. From the result, uniRDMA's performance is close to native RDMA in hybrid virtual environment and the overhead is less than 5\% to hardware-based virtualization. uniRDMA also has high scalability and adapts to real-world RDMA applications in hybrid cloud environments. The main contributions in this paper are as follows:

\begin{itemize}
\item We proposed vRNIC, which is a complete virtual RDMA device in host user-space for high flexibility and hardware independency. vRNIC is unified for both VMs and containers with specific driver. Meanwhile, the performance is optimized by mapping vRNIC to physical RNIC same as native RDMA.

\item We proposed a virtual layer for vRNICs, which is responsible for vRNICs' instantition and mapping to physical RNIC. Also, virtual RDMA network are configured in the virtual layer.

\item The whole unified framework, namely uniRDMA, is evaluated and the results proved that uniRDMA maintains high performance close to native RDMA.
\end{itemize}

The paper is organized as follows. Section 2 describes the backgroud of uniRDMA designs. Section 4 describes the vRNIC design in uniRDMA, and Section 5 introduces the design of virtual layer. Then, Section 6 reports the experimental results. Section 7 introduces related work. Finally, Section 8 concludes our work.
