\section{Introduction}
RDMA (remote direct memory access) is a high-performance network,  with high throughput, low latency and low CPU load. Thus, RDMA is widely adopted in supercomputing for various applications,  such as artificial intelligence, and big data processing. 

Recently, migrating to cloud is a trend for supercomputing users. Different from traditional HPC environment, resource provisioning are done in the form of VMs or containers in clouds~\cite{hpc-cloud}. Moreover, hybrid virtual environments are common for clouds. In specific, VMs, containers or other form of virtualization (e.g. containers in VMs) can be found in the same datacenter or even on the same host machine. For hybrid virtual environments in clouds,  to exploit RDMA resources, RDMA virtualization needs to meets four following goals:

\begin{itemize}
	\item {\verb|Unification|}: Only single RDMA virtualization framework needs to be deployed in hybrid virtual environments and it provides centralized management. If not, there may cause the low resource utilization or higher management complexity.
	\item {\verb|High performance|}: Performance of virtual RDMA should be close to native RDMA in terms of throughput, latency, or CPU load. Meanwhile it should suit for large-scale virtual cluster.
	\item {\verb|Compatibility|}: RDMA virtualization framework should be clean and stable for different environments, and has minimal dependence of others, e.g. RDMA kernel driver.
	\item {\verb|High manageability|}: Basic management should be meet for the clouds, such as, performance isolation, virtual network management or portability.
\end{itemize}

However, neither of existing solutions can simultaneously achieve above goals. These solutions include software-based and hardware-based solutions. The software-based solutions are mainly about HyV~\cite{pfefferle2015hybrid}, FreeFlow~\cite{kim2019freeflow} or MasQ~\cite{he2020masq}. FreeFlow is  designed for containers and HyV/MasQ are proposed for virtual machines. All of them are designed for specific virtual environments. Moreover, in FreeFlow, there are apparent overhead in RDMA network because all RDMA commands do not bypass the router and kernel; for HyV /MasQ, their virtual layers are in kernel-space that brings new problems: the kernel's attack surface is larger due to lots of inserted code in kernel, management development is inflexible in kernel programming, and the inserted modules are dependent on hardware-specific RDMA kernel drivers.

The hardware-based solution is based on PCI pass-through for less overhead, like SR-IOV~\cite{sr-iov}. For hardware-based solutions (e.g. SR-IOV), RDMA device resources are directly allocated to virtual machines or containers. Thus, the device utilization is static and inefficient. Moreover, RDMA networks in clouds are still managed by physical switches or routers. Thus, virtual network management is not scalable and portable in large-scale clouds.

To address these problems, we propose a unified RDMA virtualization framework for hybrid virtual environment in clouds, namely uniRDMA, also with the design goals of flexible management and high-performance. uniRDMA consists of multiple vRNICs and management layer in each host server.

As the basic abstract unit of uniRDMA, each vRNIC consists of backend and frontend. The backend is virtualized device in user-space with basic RDMA attributes, such as QPs (Queue Pairs) and MRs(Memory Regions). Thus, vRNIC is compatible for different RDMA kernel drivers. The frontend forwards RDMA commands to backend, and it is unified for VMs and containers through the same communication protocol. Moreover, to optimize the performance, RDMA resources are mapped between guests (VMs or containers) and backends. The management layer tacks charge of vRNICs in each host server, including management RDMA devices, configuring vRNIC address and routing rules. 

Finally, we implement the prototype and evaluate it in different aspects, such as throughput, latency, scalability, with and real-word applications. From the result, uniRDMA's performance is close to native RDMA in hybrid virtual environment and the overhead is less than 5\% to hardware-based virtualization. uniRDMA also has high scalability and adapts to real-world RDMA applications in hybrid cloud environments. The main contributions in this paper are as follows:

\begin{itemize}
\item We proposed vRNIC, which is a para-virtualization RDMA device in host user-space for high flexibility and hardware independence. vRNIC is unified for both VMs and containers with specific driver. Meanwhile, the performance is optimized by mapping vRNIC to physical RNIC same as native RDMA.

\item We proposed a management layers for vRNICs, which is responsible for vRNICs' instantition and mapping to physical RNIC. Also, virtual RDMA network are configured in the virtual layer.

\item uniRDMA, is implemented and evaluated. The results proved that uniRDMA maintains high performance close to native RDMA.
\end{itemize}

The paper is organized as follows. Section 2 describes the background of uniRDMA designs. Section 4 describes the vRNIC design in uniRDMA, and Section 5 introduces the design of virtual layer. Then, Section 6 reports the experimental results. Section 7 introduces related work. Finally, Section 8 concludes our work.
