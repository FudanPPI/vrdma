\section{Implementation}
%我们在Linux环境中实现了URN框架：
 We implement a prototype for URN in Linux environments, the specific kernel is CentOS 7.4.1708 (Linux 3.10.0-693.el7.x86\_64). The hypervisor of VM is KVM and the container engine is Docker. We use the hardware-independent verbs library version is libibverbs-41mlnx1. In specific, the RDMA device is Mellanox ConnectX-3 (56 Gb/sec) and corresponding hardware-specific library is libmlx4-41mlnx1. In this section, we introduce some representative implementation details:
 
 % 资源映射：根据资源所属于的物理地址空间，RDMA通信过程中主要有两类资源：内存资源（QP等），和设备资源（门铃寄存器）.
 \subsection{Resource Mapping}
According the physical address space, there are mainly two kinds of resources for RDMA: memory resources (e.g. QPs, MRs) and device resources (e.g. Doorbell registers). 
 
 % 对于内存资源: vRNIC后端需要将RDMA资源映射给guest 应用程序的虚拟地址。在这个过程中，应用需要将地址信息传递到后端，然后由后端根据对应的物理页面创建RDMA资源。在虚拟机中, 利用vhost-user的半虚拟化技术构建vRNIC后端，vRNIC后端可以与虚拟机共享整块内存。 在具体操作中，需要先借助vRNIC前端将GVA转换为GPA；然后，在vRNIC后端中，由于vRNIC后端已经和虚拟机共享整块内存，因此，GPA可以被线性转换为HVA。但是，从GVA到GPA的转换由于guest OS的内存管理， 可能存在非线性的不连续现象，进一步导致转换的HVA区域也是不连续的。此时，在后端可以利用mremap系统调用重新映射虚拟内存，以将对应的物理页面映射到连续的HVA区间,在通过修改的verbs库利用该虚拟地址创建RDMA资源。
 For memory resources: in VM, we utilize the vhost-user para-virtualization for vRNIC backends, and shared physical memory between vRNIC backends and guest. The guest virtual address (GVA) of guest RDMA resources are translate to guest physical address (GPA) in vRNIC frontend, and GPA is translated to host virtual address (HVA) in vRNIC backend. In specific, the HVA may be discontinuous and we remap it through simple system call.
 
 % 对于容器，可以直接通过IPC的共享内存来完成内存资源的映射，此时，不存在GHA和GPA的地址翻译问题，另外也不存在映射过程中的地址不连续问题。相应的，在容器的前端中，通过共享内存，将应用的RDMA资源重新映射到后端已创建RDMA资源的物理页面。对于不同的RDMA资源，容器和后端使用的共享文件路径是不同的，从而确保了RDMA资源映射内存的正确性，也确保了不同vRNIC后端的RDMA资源是隔离的。
 In containers, there are not GPA translation and discontinuous address problems, and we map resources through the shared memory IPC directly. Note that different RDMA resources, they use different shared memory file to finish the mapping. Thus, the mapping is isolated and security.
 
 %对于设备资源（例如门铃），本文采用了另一种虚拟化技术vhost，vhost 后端作为一个helper模块位于主机内核空间，负责完成虚拟机和容器对应虚拟地址到设备门铃寄存器的映射。在虚拟机中，对于上述地址翻译过程，直接通过vhost后端进行，并通过MMIO映射到设备的物理页面。对于容器场景，本文设计了一个自定义的设备接口，用来连接容器的vRNIC前端和对应的vhost后端, 容器通过该设备接口传输虚拟地址到vhost后端，再让vhost后端完成映射。
 For RDMA device resources: we design another utilize another para-virtualization vhost, vhost backend is as a helper module in host kernel space. For VMs, in vhost backend, the above address translation are finished  and remap the HVA to physical device resource (e.g. DoorBells) through MMIO. For containers, we design a costumed device interface to connect vRNIC frontend to the vhost backend for above mapping.
 
 % RDMA数据路径
 % 在原生RDMA中，应用需要往WQE中写入本地或远端的内存地址，然后，网卡会根据MTT去找到对应的物理页面进行数据传输，MTT是在注册内存时形成的。然而，在URN中，内存注册由后端完成，因此MTT是HVA。如果guest的应用程序，直接往QP中写入GVA， 物理网卡无法找到对应的物理页面，会中断数据传输。因此，在数据路径中，需要往WQE中写入翻译的地址，才能确保正常的RDMA传输。
\subsection{RDMA Data Path}
In RDMA verbs, applications needs to fillwith local or remote memory address to the work request in QPs. Then, the RNIC can find corresponding physical memory page through the MTT(Memory Translation Table), which is recorded when the memory is registered. However, in URN, the memories are registered into physical RNIC at host's vRNIC backends and the address recorded in MTT is also HVA. For applications in guest environments, if we directly fillwith GVA in QPs, the physical RNIC cannot find the corresponding physical pages and will abort this work request. Thus, the memory address translation for work request in data path is necessary. 

 
%主要挑战是如何最小化地址翻译的开销。从HVA到GVA的转换关系在vRNIC后端维持，并且在URN core中有一个包含远端地址映射关系的全局地址翻译表。地址翻译应该尽可能在guest中执行。因此，在guest中维持了一个地址翻译表，对于本地地址，直接在注册内存时将地址翻译信息写入表中。对于远端地址，我们利用了RDMA数据路径中工作请求的局部性，只在第一次使用远端地址时向URN core获取地址映射，后续的操作可以直接使用已经缓存的远端地址映射关系。这样做的好处，是维持了内存消耗与延迟之间的平衡，不需要耗费内存存储冗余的地址映射关系，也不需要每次都转发到URN中执行。
The main challenge of memory address translation is how to minimal the overhead. Recall the local mapping for HVA and GVA is maintained in the vRNIC backend, and all mapping (including remote address) is centralized in the URN core for each server. Apparently, the memory address translation should be executed in guest, instead of vRNIC backends. 

Thus, we maintain a memory address mapping table in guest. In specific, the table locates in hardware-specific library. For local memory address, the mapping can be filled into tables when the memory is registered. However, for remote memory address, the guest is unaware of remote memory register and the guest should request the URN core to get the mapping relationship.Fortunately, the RDMA work request has strong locality because the address is often in the same registered memory region. Thus, we utilize the cache mechanism, at first, the vRNIC frontend request the backend to get remote address translation, and then, the memory mapping are maintained for subsequent data commands. Thus, there are no redundant memory address mapping in the table and only apparent latency in the first time. This mechanism make a trade-off between memory consumption and latency.

