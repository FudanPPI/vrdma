\section{Implementation} \label{impl}
We implement the \sys system in Linux environment. The hypervisor of VM is QEMU/KVM and the container engine is Docker. The vRNIC frontends for VMs and containers are implemented differently. The \sys Core~(with vRNIC backends) includes about 2000 lines of code.
And in the management node, Zookeeper~\cite{zookeeper} is used to keep the information for RDMA resources~(such as RDMA address mappings and the numbers of VFs) and the policies~(such as QP limit and QoS).
In addition, we modified the \textit{libibverbs-41mlnx1} Verbs library, the \textit{librdma-cm} Verbs library for connection management, and the \textit{libmlx4-41mlnx1} user-space RNIC driver.
And some minor modifications are made to the host OS kernel, such as caching GPA into RNIC and mapping DoorBells. In this section, some implementation details are introduced.

% 资源映射：根据资源所属于的物理地址空间，RDMA通信过程中主要有两类资源：内存资源（QP等），和设备资源（门铃）.
\subsection{Resource Mapping}

There are mainly two kinds of resources for RDMA: memory resources~(e.g., QPs, MRs) and device resources~(e.g., DoorBells). 

% 对于内存资源: 在虚拟机中, 我们利用vhost-user的半虚拟化技术构建vRNIC后端，vRNIC后端可以与虚拟机共享整块内存。 在具体操作中，需要先借助vRNIC前端将GVA转换为GPA；然后，在vRNIC后端中，由于vRNIC后端已经和虚拟机共享整块内存，因此，GPA可以被线性转换为HVA。但是，从GVA到GPA的转换由于guest OS的内存管理， 可能存在非线性的不连续现象，进一步导致vRNIC后端中映射的HVA区域也是不连续的。此时，在后端可以利用mremap系统调用重新映射虚拟内存，以将对应的物理页面映射到连续的HVA区间.

\begin{figure}[!ht]
	\centering
	\includegraphics[width=1\linewidth]{images/qp-map.png}
	\caption{QP mapping for the VMs}
	\label{fig:qp-map}
\end{figure}

For memory resources, the mapping in VMs is shown in Figure~\ref{fig:qp-map}. \sys maps the whole physical memory of the VM to the memory space of the corresponding vRNIC backend. Different processes can map to the same physical memory through the shared memory file handle. As shown in Figure~\ref{fig:qp-map}, the detailed steps are as follows: (1) The application in the guest allocates the QP buffer, and the buffer is using the shared memory in the host. The physical pages in the guest are pinned in the guest kernel. (2) The vRNIC backend receives the GPA of the QP buffer. Note that the GPA may not be contiguous. If we directly map the shared memory file through dis-contiguous GPA (offsets), the obtained HVA are also discontiguous and cannot be used as the QP buffer. Thus, the vRNIC backend remaps the corresponding shared memory to a new contiguous virtual memory, and uses it as the virtual address of the QP buffer.



The mapping in containers is simple. The application in the container explicitly allocates the shared memory as the QP buffer in the \sys-modified Verbs library. The vRNIC backend directly use the memory region by mapping the corresponding shared memory file.


\begin{figure}[!ht]
	\centering
	\includegraphics[width=1\linewidth]{images/db-map.png}
	\caption{DoorBell mapping}
	\label{fig:db-map}
\end{figure}

For RDMA device resources, the mapping in VMs is as shown in Figure \ref{fig:db-map}. The detailed steps are as follows: (1) The application in the guest requests the DB HPA handle to the vRNIC backend. (2) The vRNIC backend receives the request and maps the physical DoorBell. (3) The vRNIC backend obtains the DB HPA handle through the helper module in the host OS kernel and returns the handle to the application in the guest. (4) The vRNIC backend maps a virtual DoorBell, forwards the GPA and DB HPA handle to the helpe module in the host OS kernel, and the helper module translates the GPA to VM HVA. (5) The helper module remaps the VM HVA to the DB HPA. The helper module is implemented based on the para-virtrualization vhost modules.

For the containers, the mapping of DoorBells is similar as for the VMs. The only difference is that the application in the container directly forwards the virtual memory address to the helper module in the host OS kernel, through a simple customized device driver interface.


%\subsection{Register MR in the Host}

%When vRNIC backend receives the command to register MR, it should use the GVA as the virtual address when caching the page table for physical RNIC. The physical RNIC can directly access the physical pages through the GVA in the work request of the application, instead of translating the GVA to the HVA on the data path. 

%To implement it, we modified the OFED driver and the Verbs library in the host: the OFED driver can receive the GVA parameter when register MR, and fillwith the cached page table with the GVA as the virtual memory address.


% discussion
% 随着云计算环境的复杂化，不同的需求越来越多，对RDMA虚拟化提出了更多的要求。本节讨论了RDMA虚拟化面对下列需求可能的解决思路：
\subsection{Discussion}

%As cloud computing environments become more complex, there are more and more requirements to RDMA virtualization. In this section, we discuss some possible solutions for following demands:

%（1）虚拟机迁移： 容器或虚拟机迁移在云中有很多好处，例如资源利用和故障转移。迁移分为脱机迁移和动态迁移。在脱机迁移中，虚拟机或容器关机后，迁移到远端后重新启动。因为\sys Core维护了网络地址的映射关系，该关系可以直接发送给迁移后节点的\sys Core，无需修改guest 中RDMA应用内部的网络地址。对于动态迁移，一般都需要借助AccelNet方案【引用】，应用释放RDMA资源，使用TCP/IP网络完成迁移，然后显式地重新建立RDMA连接。对于这种方式，我们也可以无缝支持。

%Virtual Instances Migration: The migration of the containers and VMs has many benefits in clouds, e.g., resource utilization and fail-over. With the virtual RDMA network, \sys can support offline migration without reconfiguring the physical RDMA network for applications. In specific, after rebooting the migrated virtual instance, the application can rebuild the RDMA connection using the same network address. The only work is to modify the address mapping in \sys Core. For live migration, currently it is still not solved because changes in memory of the virtual instance needs to tracked during live migration. This is usually done by tracking the dirty-bit in the page tables. RDMA operations bypass the tracking mechanism and break the live migration process.

% （2）RDMA软件栈对虚拟化的扩展：在RDMA虚拟化中，QP/CQ等RDMA资源需要进行映射，避免额外的数据拷贝及切换开销。现有的RDMA软件栈没有对RDMA资源映射的支持。为此，我们提出了下述建议，以加强现有RDMA软件栈对RDMA虚拟化的支持。基于这些建议，虚拟机或容器应用和虚拟层在用户空间便可以完成便捷、高效地完成MR、QP/CQ等资源的映射。具体建议如下：

%In \sys, we have to modified the Verbs library in the containers so that the RDMA Verbs requests made to the kernel drivers can be redirected to the vRNIC frontends.
%Note that Verbs APIs in the guest environment are not modified so that \sys is transparent to the guest RDMA applications.

In \sys, a few APIs in the Verbs library in the host user space are modified to support specifying a given memory address to be registered to the physical RNIC devices as key resource memory, such as QP and MR. Currently, vanilla Verbs APIs don't allow applications to specify the address of resource memory region. But this capability is essential for \sys to establish a zero-copy data path.
In order to achieve a formal solution and make the Verbs library to better support the virtualized RDMA stack, we propose the following modifications to extend the existing Verbs library.

% (1) 对于QP/CQ: 对于容器应用，在用户态驱动调用mlx4_alloc_buf分配QP/CQ的buffer时， 建议在其中增加使用共享内存的选择。对于虚拟层，在分配qp/cq资源buffer时，建议在用户态驱动调用新的API alloc_assigned_buf代替原有的mlx4_alloc_buf。这个新增的API，将为QP/CQ资源分配指定内存地址的buffer, 其中的地址参数由映射共享内存得到。

\subsubsection{\textbf{For QP/CQ}}
\
\noindent

For the Verbs library used in the containers: When \texttt{mlx4\_alloc\_buf} API is called to allocate QP/CQ buffer, it is recommended to add an option to use shared memory in the parameters instead of using normal memory.

For the Verbs library used in the host: When vRNIC backend creates QP/CQ, a new API \texttt{alloc\_assigned\_buf} is suggested that instead of existing \texttt{mlx4\_alloc\_buf} API. This new API will use the memory address specified in the parameters to be used as the QP/CQ buffers instead of allocating a new memory.

% (2) 对于MR: 对于容器应用，我们建议应用使用共享内存注册MR。如果已有的应用未使用共享内存注册MR，在Verbs库ibv_reg_mr中，建议增加重新映射MR buffer到共享内存的选项。对于虚拟层，为了让虚拟机RDMA应用的数据路径与原生一致，需要将虚拟机MR的页表缓存到物理网卡中，以支持虚拟机RDMA应用的数据路径。为此，我们建议虚拟层Verbs中增加一个ibv_reg_guest_mr API. 该API比原生的注册MR函数ibv_reg_mr多一个参数，GVA，用以将GVA传递到RDMA内核OFED模块。同时，原生OFED模块也需进行适配性的改动，以通过GVA参数，将虚拟机MR的页表缓存到物理网卡。 经过以上扩展，数据路径上，虚拟机应用可以直接使用本地地址GVA，无需转换为虚拟层MR的地址HVA，也不用缓存本地/远端MR GVA和HVA之间的映射关系。

\subsubsection{\textbf{For MR}}
\
\noindent

For the Verbs library used in the containers: We recommend that applications register MR in the shared memory region. For existing applications who does not use shared memory for MR, we recommend to add an option for remapping the MR buffer to shared memory in the Verbs API \texttt{ibv\_reg\_mr}.

For the Verbs library used in the host: It is desired to register the GVA and HPA of the MR to the physical RNIC, so that when the RDMA application specifies the virtual address of a data, the physical RNIC can lookup the physical address and send the data.
The existing \texttt{ibv\_reg\_mr} has a parameter specify the virtual address of the MR. Then the RDMA kernel driver will lookup the corresponding physical address by walking the page table of the current process.
In \sys, the current process that calls the \texttt{ibv\_reg\_mr} API is the vRNIC backend. It uses a different page table than the RDMA application in the VM or container. So the vRNIC backend is not able to tell the RNIC what HPA should be used for the MR.
We suggest to add an \texttt{ibv\_reg\_guest\_mr} API to the Verbs library. This new API has an additional parameter "GVA" than the existing API \texttt{ibv\_reg\_mr}. As the vRNIC backend also maps the MR in its memory space, it knowns the HVA of the MR. When calling the \texttt{ibv\_reg\_guest\_mr} API, both the GVA and HVA are passed as parameters. After receiving the GVA and HVA, the RDMA kernel driver needs to lookup the HPA using HVA by walking the page tables. Then it can register the MR to the physical RNIC using the {GVA, HPA} pair, instead of {HVA, HPA}.
With this extension, the applications in the guest can use its GVA directly on the data path.
