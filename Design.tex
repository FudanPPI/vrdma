% ********* 中文 ********%%%
% 第4章 设计
\section{Design}

% URN的设计目标是为提供一个统一的RDMA虚拟化框架，以适应混合虚拟化环境，同时实现统一的灵活的RDMA资源管理，并具备与原生RDMA接近的性能。此外，URN应尽可能确保最大的兼容性，对已有RDMA应用透明。
The goals of URN are to provide a unified RDMA virtualization framework for the hybrid virtual environment, meet the unified flexible RDMA resource management and make the performance of virtual RDMA close to the native RDMA meanwhile. Besides, URN should ensure maximum compatibility and be completely transparent to the application. 

%URN主要包含了vRNIC，URN core和mgmt center。vRNIC是对虚拟机和容器通用的虚拟RDMA网卡设备，URN core则是各主机服务器上的用于集中管理RDMA资源的虚拟化层, mgmt center可以跨多主机管理协调RDMA网络的控制策略。在本章，我们介绍了URN中的一些关键性设计工作。
To achieve the above goals, URN mainly consists of the vRNICs, the URN core and the management center. vRNIC is a virtual RDMA network device that is common to virtual machines and containers, URN Core is a virtualization layer for centralized RDMA resource management on each server, and the management center is configured with multiple control policies. In this section, we introduce some key designs in URN.

% 4.1  vRNIC设计
\subsection{vRNIC}

% vRNIC是一个虚拟RDMA设备，由前后端驱动组成。后端在主机端，模拟RDMA设备；前端在guest端, 作为vRNIC的接口，转发guest应用的RDMA 命令到vRNIC后端。
The vRNIC is a virtual RDMA device that consists of a frontend and backend driver. The vRNIC backend is in the host and emulates the RDMA device in the host. As the interface of vRNIC, the vRNIC frontend is in the guest and forwards the RDMA commands from the upper-level RDMA software to the vRNIC backend. 

% 4.1.1 vRNIC后端
\subsubsection{vRNIC Backend}

% vRNIC后端被放在主机用户空间，因为vRNIC后端在用户空间有诸多好处，例如，更小的攻击面，更高的管理灵活性，以及对内核API无依赖等，具体细节可以见2.1节。
The vRNIC backend is placed in the host user-space, because there are many benefits in user-space, which is discussed in detail at section 2.1.

% vRNIC作为一个虚拟RDMA设备，vRNIC后端需要维护关于RDMA网卡的硬件属性，与物理RDMA网卡对应，例如RDMA地址vGID，RDMA设备类型等。此外，在vRNIC的运行过程中，vRNIC后端中需要维护三方面的上下文：
% 1，来自guest的虚拟RDMA上下文信息，主要记录了虚拟RDMA资源的信息；
% 2，由物理网卡创建的RDMA上下文，包含各RDMA资源，由vRNIC后端通过调用verbs库创建和维护；
% 3，虚拟RDMA上下文到物理RDMA上下文之间的对应关系。对应关系是一对一的，因为一对一是RDMA性能隔离的基础。
As a virtual RDMA device, the vRNIC backend needs to maintain the hardware properties that correspond to the physical RNIC, such as the RDMA address GID, the RDMA device vendor, etc. In addition, three aspects of context need to be maintained in the vRNIC backend at the run-time:
(1) Virtual RDMA context information from the guest, mainly including the information about RDMA resources in the guest;
(2) The RDMA context created on the physical RNIC,  including various RDMA resources, which is maintained in the vRNIC backend with the help of the Verbs libraries;
(3) The mapping between the virtual RDMA context and the physical RDMA context. The mapping of RDMA resources is one-to-one,  because one-to-one is the base of performance isolation..

% 如果guest中所有RDMA操作均转发到vRNIC后端执行，将会引入额外上下文切换及数据拷贝，例如，MR中的数据内容或QP中的工作请求等，从而导致RDMA性能下降。考虑到控制路径仅涉及到初始化和连接建立的过程，其开销是一次性的，而数据路径是影响RDMA性能的关键。因此，为了提高RDMA网络性能，guest中RDMA资源与vRNIC后端完成了映射,数据路径直接使用映射的RDMA资源，其命令未转发到vRNIC后端。具体来说，vRNIC后端与guest共享了一些内存区域，vRNIC后端使用该共享内存创建RDMA资源并映射给上层的guest应用，包括MR、QP、CQ和DoorBell等。然后，guest应用基于这些映射的RDMA资源执行数据路径的操作，这些操作会被RDMA物理网卡执行，不用经过vRNIC后端和虚拟机内核。因此，避免了额外的数据拷贝和进程切换，从而实现了与原生RDMA相当的性能。
If all RDMA operations in the guest are forwarded to the vRNIC backend, the additional context switching and data copy will be brought, such as the content in MR or the work request in QP. As a result, RDMA performance will degrade. In RDMA, the control path only includes the initialization and the establishment of connection, its overhead is one-time, while the data path is the key to RDMA performance. Therefore, to improve RDMA performance, the RDMA resources is mapped between the vRNIC backend and the application in the guest, the data path in the guest is directly based on mapped RDMA resources, not forwarded to the vRNIC backend. Specifically, there are shared memory regions between the vRNIC backend and the application in the guest, the vRNIC backend creates RDMA resources with the shared memory and maps them to the application in the guest, including MR, QP, CQ, Doorbell. Then, The application in the guest executes the data operations through these mapped RDMA resources. The data operations are performed in the physical RNIC without the vRNIC backend and the kernel of the guest. Thus, the additional context switching and data copy are avoid, and the performance is comparable to the native RDMA.

% 4.1.2 vRNIC前端
\subsubsection{vRNIC Frontend}

% 在虚拟机或容器中，vRNIC前端都是连接虚拟机或容器应用到vRNIC后端，但是它们的设计上有差别。 
% 在虚拟机场景中，vRNIC后端模拟了一个外部设备，由虚拟机OS中的vRNIC前端驱动。原生RDMA内核驱动包括与设备无关的OFED模块和设备相关的模块，OFED模块接收verbs操作并转化为内核层级的RDMA 命令；设备相关模块注册具体接口到OFED，并通过物理RNIC执行这些调用。对应的，在虚拟机中为了使用vRNIC，一个自定义的设备相关的驱动模块被构建在虚拟机内核，用来提供与原生类似的设备相关接口并注册它们到OFED模块。OFED中的RDMA命令将通过这些设备相关接口，被转发到vRNIC前端模块，再从前端转发到vRNIC后端去执行。因此，原生的RDMA应用无需修改便可运行在虚拟机中，而且虚拟机内核仍然可以使用RDMA。
% 在容器场景中，应用和vRNIC后端共享主机内核，仅位于不同的命名空间。为了在容器中使用vRNIC，容器的verbs库中与内核RDMA驱动交互的接口，被替换为vRNIC前端接口。Verbs操作被劫持到vRNIC前端,并被转发到vRNIC后端。
In both virtual machines and containers, the vRNIC frontend connects the application in the guest to the vRNIC backend, but the designs are different. 
In virtual machines, the vRNIC backend emulates as an external device, driven by the vRNIC frontend in the guest kernel. The native RDMA kernel driver includes device-independent modules (OFED) and device-specific driver. OFED accepts the Verbs operations and translates them into the kernel-level RDMA commands. The device-specific driver registers the specific interface into OFED and executes these calls via physical RNIC. Correspondingly, to use vRNIC in virtual machines, a custom device-specific driver is built to provide the similar device-specific interfaces and register them into OFED. But RDMA commands in the OFED will be forwarded to the vRNIC frontend via the registered device-specific interfaces; from there
they are further forwarded to the vRNIC backend to be executed. As a result, RDMA applications can run in the virtual machine without modification, and RDMA can be used in the guest kernel. 
In containers, the application shares the host kernel with the vRNIC backend, only in different namespace. To use vRNIC in the containers, the Verbs library interfaces that interact with the RDMA kernel driver are replaced with the vRNIC frontend interface. The Verbs operations are hijacked to the vRNIC frontend and further forwarded to the vRNIC backend.

% 4.1.3 Verbs库和用户态驱动 
%【问题1： Verbs库和用户态驱动改动既包括guest（虚拟机和容器）也包括host（虚拟层的），按何种顺序介绍最佳？先总体介绍guest的修改目的，再讲虚拟机和容器分别作的具体修改，然后介绍host端的修改。】
%【问题2：缩写厘定，gva还是gvm，hva，hvm，s-hva还是s-hvm？】
\subsubsection{Verbs library and user-space driver}

% Verbs库为RDMA应用提供了统一的Verbs接口，在控制路径上，Verbs库接受RDMA应用的Verbs命令并与RDMA内核驱动进行交互，在数据路径上，Verbs库调用设备相关用户态驱动，绕过了操作系统内核。为了让虚拟机或容器中的应用使用vRNIC，同时维持vRNIC对应用的透明性，Verbs库及其相关驱动需要适配vRNIC架构并提供与原生一致的Verbs接口。
The Verbs library provides the unified Verbs interface for RDMA applications. In the control path, the Verbs library forwards Verbs from RDMA applications to the RDMA kernel driver. In the data path, the Verbs library calls the device-specific user-space driver, which bypasses the kernel. To use the vRNIC in virtual machines or containers, and make it transparent to applications, the Verbs library and driver to be adapted to the vRNIC while keeping the same Verbs interface as native RDMA.


% 在容器中，为了让Verbs转发到vRNIC后端，将Verbs库中到内核驱动的接口替换为到vRNIC前端交互的接口。此外，为了让容器应用与vRNIC后端之间完成RDMA资源的共享内存映射，包括MR、QP/CQ和DoorBell，具体修改如下：
%（1）针对MR：在原生RDMA中，MR的buffer是由应用指定的，并且通常是私有内存。在URN中，容器的Verbs用户库中将应用指定的MR内存重新映射到指定的共享内存，然后将该共享内存信息随Verbs命令及参数传递到vRNIC后端。vRNIC后端可以利用这一共享内存创建MR。
%（2）针对QP/CQ：原生RDMA中QP/CQ内存的分配是在用户态驱动中进行的，也不是共享内存。在URN中，我们在容器的用户态驱动中增加了分配共享内存作为QP/CQ buffer的函数（alloc_shm_buf，参加4.5节）。对应的共享内存信息会随Verbs命令和参数传递到vRNIC后端。
% vRNIC后端在创建QP/CQ时，将分配这一共享内存作为QP/CQ的buffer。为此，我们增加了另一个新的函数（alloc_assigned_buf，参见4.5节），其指定具备特定地址的一块内存作为QP/CQ的buffer。
In the container, to forward the Verbs calls to the vRNIC backend, the interface to the RDMA kernel driver in the Verbs library is replaced to the vRNIC frontend's. In addition, to achieve the shared memory mapping about RDMA resource  between the application and the vRNIC backend, including MR, QP/CQ and Doorbell, there are more modifications in the Verbs library and user-space driver as follows:
(1) MR: In native RDMA, the MR buffer is assigned by the application and is generally private. In URN, the container's Verbs library will remap the gvm(guest virtual memory) of the MR buffer to the shared physical memory. And the shared memory information will be forwarded to the vRNIC backend along with the Verbs call and parameters. The vRNIC backend can get the s-hvm (shared host virtual memory) to register the MR.
(2) QP/CQ: In native RDMA, the QP/CQ buffer is allocated in user-space driver and not shared. In URN, we added a new function (alloc\_shm\_buf, see section 4.5) into the container's user-space driver to allocate the shared memory as QP/CQ buffer. The shared memory information will be also forwarded to the vRNIC backend along with the Verbs call and parameters. When the vRNIC backend creates QP/CQ, it will assign this shared memory as QP/CQ buffer. To achieve it, we add another new function (alloc\_assigned\_buf, see Section 4.5) in the user-space driver which assigns a memory piece with the specific address as QP/CQ buffer.

%（3）针对DoorBell：在原生RDMA中，verbs库打开RDMA设备接口，经RDMA内核驱动映射门铃到应用程序。在URN中，Verbs库打开虚拟的设备接口，并通过vRNIC后端和内核辅助模块来映射门铃。具体细节在第5节介绍。
(3) DoorBell: In native RDMA, the RDMA device is opened in the Verbs library and DoorBell is mapped to the applications through the RDMA kernel driver. In URN, the container's Verbs library opens the virtual device interface and maps the DoorBell through the vRNIC backend and a costumed kernel module. The details will be introduced in Section 5.

% 在虚拟机中，vRNIC在实例化时已经共享了虚拟机的整块物理内存空间。即使虚拟机应用使用默认的方式分配MR、QP或CQ等RDMA资源的内存，无需修改，vRNIC后端可以获取对应的共享内存块。具体地，通过虚拟机内核的vRNIC前端驱动中，将这些buffer的GPA（虚拟机物理内存信息）随RDMA命令和参数转发给vRNIC后端，vRNIC后端可以将GPA翻译为HVA，并使用这些共享内存块作为RDMA资源的buffer。
In the virtual machine, the whole physical memory of the virtual machine is already shared with the vRNIC backend when the vRNIC instantes. Though the application defaultly allocates the buffers of MR, QP or CQ without modification, the vRNIC backend still can get the corresponding shared memory piece. In specific, through the vRNIC frontend in the guest kernel, the GPA(guest physical memory addresses of these buffers can be forwarded to the vRNIC backend along with the Verbs calls and parameters, and the vRNIC backend can translate the GPA to the HVA (host virtual memory address) and use the shared memory pieces as the buffers of RDMA resources.

% 在完成上述修改后，在数据路径中，guest的RDMA应用往QP中写入工作请求，其内存地址是虚拟机或容器应用的地址空间gvm（guest virtual memory），RDMA物理网卡将根据缓存的页表检查工作请求中的地址，但是其缓存的MR页表信息则是主机vRNIC后端的s-hvm（shared host virtual memory）。因此，为了仍物理网卡成功执行guest中的工作请求，需要对RDMA工作请求中的gvm转换成s-hvm。为此，我们在guest用户态驱动中记录有MR资源的地址映射关系，即{key, gvm, s-hvm},在数据路径中，工作请求在写入QP之前，其gvm将转换成s-hvm。
After the above modifications, in the data path, the application in the guest writes the QP buffer with a work request whose memory address is still the gvm, but the gvm address will be checked in the physical RNIC through the cached page table that is about s-hvm in the vRNIC backend. Therefore, to make the vRNIC execute the work request in the guest successfully, it is still necessary to convert the gvm to the s-hvm 
in the work request. To achieve it, we record the memory address mapping relationship of MR buffer in the guest user-space driver {key, gvm, s-hvm}. In the data path, the gvm of a RDMA work request can be translated into the s-hvm before it is written into the QP buffer.

% 基于上述修改，无论是虚拟机还是容器，RDMA控制verbs将经过vRNIC前端到达vRNIC后端，在vRNIC后端中执行操作并映射RDMA资源。基于映射的RDMA资源，数据Verbs操作不需要转发到vRNIC后端,可以直接在guest用户空间，和原生RDMA一样避免了径额外的的拷贝或切换开销。
Based on the above modifications, in both VMs and containers, the control Verbs operations are forwarded into the vRNIC backend through the vRNIC frontend, the vRNIC backend executes the operations and map the buffers of RDMA resources. Based on mapped RDMA resources, the data Verbs operations are not forwarded to the vRNIC backend, and can executed directly in the guest user-space, which avoids the cost of additional data copy or context switching as native RDMA.

% 4.2  URN Core 设计
\subsection{URN Core}

% 为了对vRNIC进行统一管理，高效利用物理RDMA网卡，我们构建了一个统一的虚拟层URN core。URN core部署于集群的每台服务器上，其主要功能包括实例化vRNIC和配置虚拟RDMA网络等。

% 4.2.1 实例化vRNIC 
% URN core在收到虚拟机或容器应用申请vRNIC的请求后，会实例化vRNIC。URN core在实例化vRNIC的主要工作包括： 初始化vRNIC属性、绑定vRNIC到物理网卡、构建vRNIC前后端消息通道以及隔离各vRNIC实例等。

% 初始化vRNIC属性：为了让vRNIC具备与硬件一致的功能接口，URN Core需要对vRNIC的不变属性进行初始化。vRNIC中的不变属性包括RDMA地址vGID，设备号等等。对于RDMA地址vGID，URN core会随机生成IPv6格式的虚拟值，同时通过mgmt center进行协调和登记，以确保各vRNIC的RDMA地址等信息是不冲突的并维护在统一的数据库中。

% 绑定vRNIC到物理网卡：vRNIC实例需要依靠物理网卡实现RDMA功能，因此，实例化vRNIC时需要将vRNIC实例与对应的物理RDMA网卡绑定。如果是多网卡设备或者多个网卡接口（如使用SR-IOV），还需确定从vRNIC到对应物理网卡接口的映射关系。

% 构建vRNIC消息通道：URN Core需要构建vRNIC后端与vRNIC前端（guest）之间的消息通道。对于容器来说，容器与vRNIC后端共享某一IPC命名空间，即可完成vRNIC前端与后端之间的消息交互。对于虚拟机来说，由于vRNIC后端在主机用户空间，而hypervisor位于主机内核空间。为了避免guest的消息先拷贝到hypervisor层再到vRNIC后端，我们在vRNIC后端与guest中的vRNIC前端之间构建一块共享内存，以此作为消息通道。具体来说，虚拟机内存区域对应的物理页面，与vRNIC后端之间是共享的。这样，guest中的任意RDMA消息，均可以直接由vRNIC后端获取，而无需进入到主机内核空间，同时也减少了消息从guest到vRNIC后端的延迟。

% vRNIC实例隔离：URN core需要确保各vRNIC之间的消息通道是隔离的。在云环境中，同一主机上可能部署多个虚拟机或容器，该主机的URN core需要实例化多个vRNIC后端，以满足不同的guest实例。因此，需要确保各vRNIC之间的消息通道是隔离的，尤其是容器，由于其隔离性不如虚拟机彻底。如果多个容器和主机共享同一文件命名空间或IPC空间，那么，很容易通过扫描获取到其他容器或虚拟机vRNIC的消息通道。URN core利用了文件命名空间的机制，文件命名空间中的文件对于其他命名空间是不可见的。URN core在实例化vRNIC时，将各vRNIC消息通道对应的文件位于不同的命名空间，从而实现了各vRNIC实例之间消息通道的隔离。

% 4.2.2 配置虚拟RDMA网络

% URN core在实例化vRNIC后，需要对vRNIC的网络进行配置，包括虚拟RDMA地址配置以及虚拟RDMA路由配置。

% 对于虚拟RDMA地址配置，URN core 在实例化vRNIC时， 给vRNIC配置的vGID是虚拟值，与物理网卡的GID无关。

% 对于虚拟RDMA路由配置，每台主机的URN core中配置了虚拟RDMA网络路由规则。具体来说，URN core在实例化vRNIC时，还给每个vRNIC都配置了组ID，路由规则的定义形式为:{group ID1, group ID2, Policy}。例如，如图4所示，容器1和容器2的vrnic被配置到同一组中，因此允许创建RDMA连接。作为对比，容器1和虚拟机1属于不同的用户组，按路由规则无法建立RDMA连接。各虚拟实例之间建立RDMA连接的具体细节可以参考4.3.1。

% RDMA网络与传统TCP不同，RDMA还支持单边操作。在guest应用的RDMA单边操作中，RDMA的工作请求中包含了远端guest MR的内存地址及key，远端MR内存地址同样需要在RDMA用户态驱动中完成转换，将远端的gvm转换为远端vRNIC后端的s-hvm。为了便于guest应用查询远端gvm对应的s-hvm，我们在URN core中建立了一个分布式的KV数据库，存储各节点注册MR的映射关系{key，gvm，s-hvm}。在guest应用执行单边操作时，将向URN core 中的分布式KV数据库查询，以获取远端对应的s-hvm，缓存到本地并写入工作请求。

% 总之，URN core中的虚拟网络配置是动态调整的，因此，容器和虚拟机迁移后无需更改物理网卡及交换机等硬件的网络配置，实现便携的虚拟机或容器迁移。基于URN core中的路由规则，可以支持云环境中的多租户隔离。而各URN core存储的MR地址映射关系，对RDMA网络特有的单边操作给予了支持。


\begin{figure}[!ht]
	\centering
	\includegraphics[width=1.0\linewidth]{images/route-config}
	\caption{Group Configuration and Routing: The vRNICs of container 1 and container 2 are configured in one group. Thus, two containers can create RDMA connections. And VM 1 are not allowed to create RDMA connections to containers in this figure because it is not added into the group. }
	\label{fig:route-config}
\end{figure}


% 4.3 虚拟RDMA workflow
\subsection{Virtual RDMA Workflow}
% 为了更好地说明虚拟RDMA的工作过程，我们介绍了虚拟RDMA的详细工作流程，如图4所示。虚拟RDMA的工作流程涉及guest中的应用、verbs库以及主机的vRNIC后端交互。整个工作流程依次分为初始化阶段、连接阶段和数据阶段。其中，初始化阶段和连接阶段属于RDMA控制路径，初始化阶段分别包括设备打开和RDMA资源创建，连接阶段中RDMA应用与远端建立连接。数据阶段中RDMA应用执行数据路径的传输操作。

\begin{figure}[!ht]
	\centering
	\includegraphics[width=1\linewidth]{images/RDMA-path.png}
	\caption{The Workflow of RDMA SEND operation}
	\label{fig:route-config}
\end{figure}

% 4.3.1 初始化阶段
%  初始化阶段主要完成资源的初始化，分别包括设备打开（I1）、注册内存buffer（I2）和创建QP/CQ等RDMA资源（I3）。具体细节如下： 

% I1（Guest应用打开vRNIC设备）：Guest应用依次调用Verbs接口ibv_get_devicelist和ibv_open_device，获取并打开vRNIC设备。 Guest中的Verbs库将传递这些命令和参数到vRNIC后端，vRNIC后端会打开在vRNIC实例化时绑定的物理RDMA设备。

% I2（Guest应用注册MR）：在容器中，Verbs库中将应用指定的MR内存重新映射到某一共享内存，然后将该路径随MR命令及参数传递到vRNIC后端，虚拟机无需此步骤，因为在实例化vRNIC时已与vRNIC后端共享整个内存。对于容器和虚拟机，vRNIC后端在收到该命令及参数后，会使用对应的共享内存注册MR，并返回MR内存地址（s-hvm）和key等信息，在虚拟机和容器的Verbs库中，则会记录guest中MR内存地址（gvm）与s-hvm，以及MR的key，这三者的对应关系，用于后续的数据路径操作。

% I3（Guest应用创建QP/CQ）：在容器中，Verbs及其设备相关库使用共享内存分配QP/CQ的buffer，对应的共享内存路径会随RDMA命令和参数传递到vRNIC后端，虚拟机无需此步骤，因为在实例化vRNIC时已与vRNIC后端共享整个内存。对于容器和虚拟机，vRNIC后端在收到这些命令和参数后，在创建QP/CQ时，使用接收的共享内存地址分配QP/CQ等RDMA资源内存，在创建资源后，vRNIC后端会返回QPN等RDMA资源的元数据。 

%4.3.2 连接阶段
% 连接阶段主要建立基于QP的RDMA连接，依次包括查询GID（C1）、交换连接信息（C2）和基于QP建立连接（C3），具体细节如下：

% C1（获取GID）：Guest应用执行ibv_query_gid，请求获取vRNIC的vGID地址，vRNIC后端在收到该命令后，会获取绑定的物理RDMA网卡的GID，并记录（vGID，GID）对应关系。不过，vRNIC后端返回给guest的仍然是vGID。

% C2（交换连接信息）：Guest应用与远端RDMA连接所需的地址信息vGID，QPN和MR key等。 该步骤完全由应用程序完成，无需依靠Verbs库和vRNIC后端。例如，应用中可以使用TCP socket来与远端guest应用完成这一系列信息的交换。

% C3（基于QP建立连接）：Guest应用将C2获取的远端vGID、QPN等信息作为ibv_modify_qp的参数,基于QP与远端建立RDMA连接。该命令转发到vRNIC后端后，vRNIC后端会先根据设置的路由表规则进行判断，如果远端vGID符合路由规则，就会将vGID转换成对应的GID，基于GID，QP ID及Key完成与远端vRNIC后端QP的配对。从vGID到GID的转换，实现了RDMA连接的网络虚拟化。

% 4.3.3 数据阶段
% 数据路径发生在连接建立后，对注册的MR中的内容执行操作。RDMA的数据路径包括图中的D1和D2。

% D1（收发数据）：Guest应用发送或接收MR中的数据。由于MR和QP已经完成了资源映射，此时，数据操作时的数据块（MR中）和工作请求（QPs）可以直接由RDMA物理网卡访问。不过，工作请求中的地址信息仍然是guest中的地址，而第三步中创建MR时，物理网卡记录是vRNIC后端中的页表信息。因此，需要借助前面缓存的三元组信息，将工作请求中的guest地址转换为vRNIC后端地址，再填充到QP中。如果是单边操作，其工作请求中还包含远端地址信息，在第一次执行命令时还需要向vRNIC后端发送请求，获取URN core中存储的远端MR地址映射关系，并缓存到本地。然后将工作请求中的本地及远端地址均进行转换，填充到QP中。

% D2（轮询CQ）：Guest应用轮询CQ获取数据操作的完成事件。由于CQ已经完成了资源映射，此时，物理网卡中工作完成的通知会直接写入到guest的CQ中。


% 4.4 mgmt center
\subsection{Management Center}

% 在URN框架里，整个集群配备一个mgmt center，作为配置管理策略，存储各节点RDMA资源信息的中心。

% 通过mgmt center，管理者向不同节点的URN core下发各种策略，包括控制平面策略，如连接管理，防火墙等；和数据平面策略，如Qos、流量计费等。管理框架图如图5所示：策略由mgmt center下发到URN core，URN core通过agent接收策略后，其中，URN core将控制平面策略配置给管理的各vRNIC后端。对于数据平面策略，URN core将其发送给各guest Verbs库中的agent，然后在各RDMA应用本地执行。

%  连接管理、防火墙等控制平面策略主要基于RDMA连接进行，而RDMA连接依靠QP等RDMA资源，因此，这些策略主要依靠RDMA资源，如QP进行控制。由于包括QP在内的所有RDMA资源均已经映射在vRNIC后端中，因此，控制策略可以直接传送到各vRNIC后端，依靠映射的RDMA资源执行。 对于数据路径，依靠监控各vRNIC后端维护的映射好的RDMA资源，如QP/CQ等，可以完成QoS等数据功能。然而，这样做将消耗额外的CPU资源而且对轮询的处理效率要求极高。因此，本文将数据策略进一步从URN core下发到各guest的用户库，在各RDMA应用本地执行，策略执行结果则可以返回给URN core或mgmt center。注意，这些扩展需要所有客户机都信任库，并且这些修改应该包含在TCB(可信计算库)中。对照native的RDMA，其数据路径绕过了操作系统内核，而RDMA物理网卡的数据管理功能及其有限，因此，其细粒度的数据管理同样需要由Verbs库配合执行。

\begin{figure}[!ht]
	\centering
	\includegraphics[width=1\linewidth]{images/urn-interface.png}
	\caption{The Architecture of URN Management}
	\label{fig:route-config}
\end{figure}

% 4.5 discussion
% 随着云计算环境的复杂化，不同的需求越来越多，对RDMA虚拟化提出了更多的要求。结合可能的需求，本节讨论了RDMA虚拟化可能面临的一些需求和解决思路：
\subsection{Discussion}

%  （1）虚拟机迁移： 容器或虚拟机迁移在云中有很多好处，例如资源利用和故障转移。迁移分为脱机迁移和动态迁移。在脱机迁移中，虚拟机或容器关机后，迁移到远端后重新启动。因为URN core维护了网络地址的映射关系，该关系可以直接发送给迁移后节点的URN core，无需修改guest 中RDMA应用内部的网络地址。对于动态迁移，一般都需要借助AccelNet方案【引用】，应用释放RDMA资源，使用TCP/IP网络完成迁移，然后显式地重新建立RDMA连接。对于这种方式，我们也可以无缝支持。
 Virtual Instances Migration: Migration of containers or VMs has many benefits in clouds, e.g. resource utilization and fail-over. With the virtual RDMA network, URN can support offline migration without reconfiguring the physical RDMA network for applications. In specific, after rebooting the migrated virtual instance, the application can rebuild the RDMA connection through the same network address. The only work is modifying the address mapping in URN core. Currently, for live migrations, it is still hard because memory regions in RDMA application may be uncertain under bypassing or one-side communication. And the problem is unrelated to URN.
 
 % （2）对RDMA虚拟化扩展的支持：RDMA虚拟化中，需要对QP/CQ等RDMA资源进行资源映射，避免额外的数据拷贝及切换开销。现有的Verbs库和用户态驱动没有对RDMA资源映射的支持。为此，我们建议Verbs库或设备相关库中增加以下API，以加强对各场景下RDMA虚拟化的支持，尤其是应用可以使用新增加的Verbs接口显式地完成MR、QP/CQ等资源的映射。如下表所示：
% API  所属库  功能 
% alloc_shm_buf()  设备相关库 创建并分组共享内存作为RDMA资源内存
% alloc_assigned_buf(addr, size) 设备相关库 为RDMA资源指定某一内存区域
% ibv_create_shm_qp/cq() 基于共享内存区域创建QP/CQ
% ibv_assigned_qp/cq 使用指定的内存区域创建QP/CQ
% ibv_reg_shm_mr() 使用共享内存区域注册MR

% 此外，为了实现高性能的数据传输，RDMA网卡缓存了应用注册内存的页表信息。在用户态虚拟化后，现有的RDMA网卡只能缓存虚拟层MR的页表信息，而无法缓存guest应用使用MR的页表信息。在URN的实现里面，尽管通过在URN core KV数据库和本地缓存数据库实现了高效的地址转换，但是引入了额外的工作量。因此，为了更好地适配虚拟化场景，建议Verbs库在创建MR时额外提供一个指定网卡缓存guest应用页表的地址参数，如原来的ibv_reg_mr(..., s-hva)扩展为ibv_reg_mr(..., s-hva, gva)。稍后，RDMA内核驱动可以根据新注入的gva参数，为该MR指定与guest应用对应的页表。

% （3）扩展虚拟Verbs接口： 目前我们设计Verbs库的主要考虑是与原生的Verbs库兼容，以实现对RDMA应用的透明性，但是，这样会导致容器应用只能基于URN verbs库使用虚拟RDMA，或者基于原生Verbs库使用RDMA，从而导致了使用RDMA的局限性。解决这一问题的可能思路是，扩展Verbs库，增加获取虚拟RDMA设备的专有Verbs接口，供应用选择调用。
%%%*********************%%%
