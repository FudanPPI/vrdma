\section{Design} \label{design}

%\sys的设计目标是为提供一个统一的RDMA虚拟化框架，以适应混合虚拟化环境，同时实现统一的灵活的RDMA资源管理，并具备与原生RDMA接近的性能。此外，\sys应尽可能确保最大的兼容性，对已有RDMA应用透明。

The design goals of \sys are: 
\begin{itemize}
	\item To provide a unified RDMA virtualization framework for the hybrid virtualization environment where VMs and containers co-exist on the same machine or in the same data center. Data center owners do not have to maintain two RDMA virtualization stacks. And a server can flexibly choose to host VMs or containers or both, without considering which RDMA virtualization stack is installed.
	\item To provide a single point of RDMA resource management for all RDMA resource users so that the resources can be optimally scheduled.
	\item To keep the performance overhead of the RDMA virtualization to the minimum.
	\item It should bring minimum impact to existing RDMA software stack and be completely transparent to the RDMA application.
\end{itemize}

%\sys主要包含了vRNIC，\sys core和mgmt center。vRNIC是对虚拟机和容器通用的虚拟RDMA网卡设备，\sys core则是各主机服务器上的用于集中管理RDMA资源的虚拟化层, mgmt center可以跨多主机管理协调RDMA网络的控制策略。在本章，我们介绍了\sys中的一些关键性设计工作。

\begin{figure}[!ht]
	\centering
	\includegraphics[width=1\linewidth]{images/framework-overview.png}
	\caption{Architecture overview of \sys. The light grey boxes are modules in \sys framework. The dark grey boxes are the memory shared by both the application and the \sys core. The shared memory can also be directly accessed by the RNIC.}
	\label{fig:framework-overview}
\end{figure}

The architecture of \sys is shown in Figure~\ref{fig:framework-overview}.
\sys consists of three parts, the vRNICs, the \sys Core and the management node. vRNIC is a virtual RDMA network device that looks like a real RDMA device inside VMs and containers. \sys Core is a virtualization layer for centralized RDMA resource management on the host machine. And the management node is a centralized management entity in the data center that keeps and coordinates management and control policies for the RDMA network.
In addition, some modifications are made to the Verbs library and device userlib in the user-space of the host and the guest to adapt to \sys.

\subsection{vRNIC}

% vRNIC是一个虚拟RDMA设备，由前后端驱动组成。后端在主机端，模拟RDMA设备；前端在guest端, 作为vRNIC的接口，转发guest应用的RDMA 命令到vRNIC后端。

The vRNIC is a virtual RDMA device that consists of a frontend and a backend. The vRNIC backend is placed in the host and emulates the behavior of a RDMA device. The vRNIC frontend is placed in the guest space that exposes RDMA device interfaces to the upper device drivers. Commands are forwarded from the vRNIC frontend to the vRNIC backend to be actually executed or emulated.

\subsubsection{\textbf{vRNIC Backend}}
\
\noindent


%\newline
% vRNIC后端被放在主机用户空间，因为vRNIC后端在用户空间有诸多好处，例如，更小的攻击面，更高的管理灵活性，以及对内核API无依赖等，具体细节可以见2.1节。

In \sys design, the vRNIC backend is placed in the host user-space instead of the host kernel-space. This is because the device emulator in the user-space can be developed, deployed, and managed more easily than that in the kernel-space. Also, its faults can be isolated from the OS kernel.

% vRNIC作为一个虚拟RDMA设备，vRNIC后端需要维护关于RDMA网卡的硬件属性，与物理RDMA网卡对应，例如RDMA地址vGID，RDMA设备类型等。此外，在vRNIC的运行过程中，vRNIC后端中需要维护三方面的上下文：
% 1，来自guest的虚拟RDMA上下文信息，主要记录了虚拟RDMA资源的信息；
% 2，由物理网卡创建的RDMA上下文，包含各RDMA资源，由vRNIC后端通过调用verbs库创建和维护；
% 3，虚拟RDMA上下文到物理RDMA上下文之间的对应关系。对应关系是一对一的，因为一对一是RDMA性能隔离的基础。

As a virtual RNIC device emulator, the vRNIC backend needs to maintain the same set of hardware properties that the physical RNIC has, such as the RDMA address GID and the RDMA device vendor. In addition, three kinds of contexts need to be maintained in the vRNIC backend at runtime:
(1) Information of the runtime RDMA resources created and used on the emulated RDMA device, such as the memory address of the virtual MR/QP;
(2) Information of the runtime RDMA resources actually created and used on the physical RNIC, such as the memory address of the real MR/QP;
(3) The mapping between virtual RDMA context and physical RDMA context.

vRNIC backends are instantiated by the \sys core. They interact with physical RNIC through unmodified verbs libraries in the host environment. vRNIC backend acts like an RDMA application in the host environment and delegates all the requests from vRNIC frontend on the control path. On the data path, the key RDMA resources of the user applications in the guest environment, e.g. QPs and MRs, are directly mapped to the physical RDMA device. During actual data transmission both vRNIC frontend and backend are bypassed on the data path.

% 如果guest中所有RDMA操作均转发到vRNIC后端执行，将会引入额外上下文切换及数据拷贝，例如，MR中的数据内容或QP中的工作请求等，从而导致RDMA性能下降。考虑到控制路径仅涉及到初始化和连接建立的过程，其开销是一次性的，而数据路径是影响RDMA性能的关键。因此，为了提高RDMA网络性能，guest中RDMA资源与vRNIC后端完成了映射,数据路径直接使用映射的RDMA资源，其命令未转发到vRNIC后端。具体来说，vRNIC后端与guest共享了一些内存区域，vRNIC后端使用该共享内存创建RDMA资源并映射给上层的guest应用，包括MR、QP、CQ和DoorBell等。然后，guest应用基于这些映射的RDMA资源执行数据路径的操作，这些操作会被RDMA物理网卡执行，不用经过vRNIC后端和虚拟机内核。因此，避免了额外的数据拷贝和进程切换，从而实现了与原生RDMA相当的性能。

When forwarding RDMA requests from the vRNIC frontend to the vRNIC backend, it causes context switching and data copy between the guest and the host. For example, when sending a data package, the data content in MR and the work request in QP need to be forwarded to the backend. This incurs performance degradation, especially on the data path. In order to achieve maximum performance, the RDMA resources are mapped in the shared memory region of the vRNIC backend and the RDMA application in the guest. On the data path, both the transmitted data and the sending/receiving requests are directly accessible to both the vRNIC backend and the RDMA applications, without an extra copy operation. Specifically, the vRNIC backend allocates RDMA resources in the shared memory region and RDMA the application in the guest maps to the same memory region in its own address space. The RDMA resources include MR, QP, CQ, and Doorbell. The RDMA application in the guest virtual environment puts data and requests in the shared RDMA memory region. The physical RNIC sees the data transmission requests in the shared memory region and performs the actual work without involving the vRNIC backend and the guest OS kernel.

The design of the vRNIC Backend is the same for VMs and containers.

\subsubsection{\textbf{vRNIC Frontend}}

\
\noindent

vRNIC Frontend is located in the guest environment acting as the NIC interface. Applications in the guest environment can use the NIC interface through Verbs library. For VMs, the frontend virtual device is in guest kernel space and is designed to expose interfaces the same way as the RDMA device driver to the upper unmodified Verbs library. For containers, the frontend module is provided as a library that is linked to the applications aloneside with Verbs library.

% 在虚拟机或容器中，vRNIC前端都是连接虚拟机或容器应用到vRNIC后端，但是它们的设计上有差别。 
% 在虚拟机场景中，vRNIC后端模拟了一个外部设备，由虚拟机OS中的vRNIC前端驱动。原生RDMA内核驱动包括与设备无关的OFED模块和设备相关的模块，OFED模块接收verbs操作并转化为内核层级的RDMA 命令；设备相关模块注册具体接口到OFED，并通过物理RNIC执行这些调用。对应的，在虚拟机中为了使用vRNIC，一个自定义的设备相关的驱动模块被构建在虚拟机内核，用来提供与原生类似的设备相关接口并注册它们到OFED模块。OFED中的RDMA命令将通过这些设备相关接口，被转发到vRNIC前端模块，再从前端转发到vRNIC后端去执行。因此，原生的RDMA应用无需修改便可运行在虚拟机中，而且虚拟机内核仍然可以使用RDMA (and RDMA can be used in the guest kernel)。
% 在容器场景中，应用和vRNIC后端共享主机内核，仅位于不同的命名空间。为了在容器中使用vRNIC，容器的verbs库中与内核RDMA驱动交互的接口，被替换为vRNIC前端接口。Verbs操作被劫持到vRNIC前端,并被转发到vRNIC后端。

In both VMs and containers, the vRNIC frontend connects the application in the guest to the vRNIC backend, but the designs are different. In VMs, the vRNIC backend emulates as an external device, driven by the vRNIC frontend in the guest kernel. The native RDMA kernel driver includes device-independent modules (OFED) and device-specific provider (?). OFED accepts the Verbs operations and the provider (?) is registered into OFED and executes these RDMA commands via physical RNIC. Correspondingly, to forward RDMA commands from OFED to the vRNIC backend, a thin virtual provider is registered into OFED. RDMA commands in the OFED will be forwarded to the vRNIC frontend via the registered device-specific interfaces; from there
they are further forwarded to the vRNIC backend to be executed. As a result, RDMA applications can run in the VM without modification. 

In containers, the application shares the host kernel with the vRNIC backend, only in different namespace. To use vRNIC in the containers, the Verbs library interfaces that interact with the RDMA kernel driver are replaced with the vRNIC frontend interface. The Verbs operations are hijacked to the vRNIC frontend and further forwarded to the vRNIC backend.

\subsection{\sys Core}

There is one \sys core instance in each host's user space. The \sys core is designed to manage the virtual and physical RDMA resources for the host, including instantiating vRNIC backends, managing virtual RDMA network configurations and policies. The \sys core also interact with physical RNIC through Verbs libraries.

% 为了对vRNIC进行统一管理，高效利用物理RDMA网卡，我们构建了一个统一的虚拟层\sys core。\sys core部署于集群的每台服务器上，监听虚拟机、容器以及原生的RDMA设备请求。之后，在\sys core中实例化vRNIC和配置虚拟RDMA网络等。

To manage vRNICs uniformly and exploit the physical RNIC, we built a unified virtualization layer, namely \sys Core. \sys Core is deployed on each server in the cluster, listening to the request of RDMA device from VMs, containers and the RDMA applications in the host. After that, the vRNICs are instantiated and configured with virtual RDMA network in the \sys core.

%【 讨论：管理服务器的所有RDMA资源，对于来自虚拟机和容器的网卡资源申请，\sys core会进行。1，发现和记录所有RDMA资源, 2，监控RDMA资源消耗，决策RDMA设备和资源分配（调度方式：动态轮询等等，调度目的：负载均衡、控制资源QoS），3,RDMA虚拟机容器的迁移信息】

\subsubsection{\textbf{\sys Core Service}}
\
\noindent

% \sys core 作为一个服务会在每台服务器上运行，统一监听虚拟机、容器和主机的获取RDMA设备的请求（如果原生的请求不进行统一处理，将会导致原生和虚拟化对RDMA设备接口的冲突，因此修改了主机获取设备Verbs ibv_get_device_list的实现，将对应请求转发到\sys core，由\sys core统一分配RDMA设备接口）。

The \sys core as a service which runs on each server and monitors the request of obtain RDMA devices from virtual machines, containers and host. (If the host requests are not processed uniformly, it will cause the RDMA devices interfaces conflict between host RDMA and virtualized. So the implementation of the Verbs ibv\_get\_device\_list is modified, the corresponding request is forwarded to the \sys core, and the \sys core uniformly allocates the RDMA device interface) .

\subsubsection{\textbf{Instantiating vRNICs}}
\
\noindent

% \sys core在收到guest中应用申请vRNIC的请求后，会实例化vRNIC。实例化vRNIC的主要工作包括： 初始化vRNIC属性、绑定vRNIC到物理网卡、构建vRNIC前后端消息通道以及隔离各vRNIC实例等。

\sys Core instantiates a vRNIC  once receiving the request from the application in the guest. Instantiating vRNIC mainly includes 4 major work: 1) initializing vRNIC attributes, 2) binding the vRNIC to the physical RNIC, and 3) building the message channel between the vRNIC frontend and backend.

% 初始化vRNIC属性：为了模拟与硬件一致的功能接口，\sys Core需要对vRNIC的不变属性进行初始化，包括RDMA地址vGID，设备号等等。对于RDMA地址vGID，\sys core会随机生成IPv6格式的虚拟值，同时登记到mgmt center的数据库中。

1) Initializing vRNIC attributes: To emulate the interface as physical RNIC, \sys Core needs to initialize the static properties of vRNIC, including RDMA address vGID, device number, and so on. For RDMA addresses vGID, \sys Core randomly generates the values in the IPv6 format and registers them into the database in the management center.

% 绑定vRNIC到物理网卡：vRNIC实例需要依靠物理网卡实现RDMA功能，因此，实例化vRNIC时需要将vRNIC实例与对应的物理RDMA网卡绑定。如果是多网卡设备或者多个网卡接口（如使用SR-IOV），还需确定从vRNIC到对应物理网卡接口的映射关系。

2) Binding vRNIC to the physical RNIC: The vRNIC instance relies on the physical RNIC to implement RDMA. Therefore, it is necessary to bind the vRNIC instance to the physical RNIC when instantiating. If there are multiple physical RNICs or interfaces (such as SR-IOV), the mapping from the vRNIC to the RNIC interface needs to be determined.

% 构建vRNIC消息通道：\sys Core需要构建vRNIC后端与vRNIC前端之间的消息通道。对于容器来说，容器与vRNIC后端共享某一IPC命名空间，即可完成vRNIC前端与后端之间的消息交互。对于虚拟机来说，由于vRNIC后端在主机用户空间，而hypervisor位于主机内核空间。为了避免guest的消息先拷贝到hypervisor层再到vRNIC后端，我们构建一块共享内存，以此作为消息通道。具体来说，虚拟机物理内存与vRNIC后端之间是共享的。guest中的任意RDMA消息，均可以直接由vRNIC后端获取，同时也减少了从guest到vRNIC后端的延迟。

3) Building the vRNIC message channel: \sys Core needs to build a message channel between the vRNIC backend and the frontend. For containers, the container can share an IPC namespace with the vRNIC backend to make the message interaction between the vRNIC frontend and the backend. For VMs, the vRNIC backend is in host user-space and the hypervisor is in host kernel-space. To avoid the messages being copied to the hypervisor and then copied to the vRNIC backend, we build a shared memory as the message channel. Specifically, the physical memory in the VM is shared with the vRNIC backend. Any RDMA messages in the guest can be fetched directly from the vRNIC backend, and the latency from the guest to the vRNIC backend is decreased.

\subsubsection{\textbf{Configuring the Virtual RDMA Network}}
\
\noindent

% \sys core在实例化vRNIC后，需要对vRNIC的网络进行配置，包括虚拟RDMA地址配置以及虚拟RDMA路由配置。
After the instantiation of the vRNIC, the \sys core needs to configure the vRNIC's network, including virtual RDMA address configuration and virtual RDMA routing configuration.
For virtual RDMA address configurations, the vGID assigned to the vRNIC by \sys Core is a virtual value, independent of the physical RNIC, when it instantiates the vRNIC.
% 对于虚拟RDMA地址配置，\sys core 在实例化vRNIC时， 给vRNIC配置的vGID是虚拟值，与物理网卡无关。

% 对于虚拟RDMA路由配置，每台主机的\sys core中配置了虚拟RDMA网络路由规则。具体来说，\sys core在实例化vRNIC时，还给每个vRNIC都配置了组ID，路由规则的定义形式为:{group ID1, group ID2, Policy}。例如，如图4所示，容器1和容器2的vrnic被配置到同一组中，因此允许创建RDMA连接。作为对比，容器1和虚拟机1属于不同的用户组，按路由规则无法建立RDMA连接。各虚拟实例之间建立RDMA连接的具体细节可以参考4.3.1。

For virtual RDMA routing configurations, virtual RDMA network routing rules are configured in the \sys core at each host. In particular, \sys Core also configures the group ID for each vRNIC when it instantiates the vRNIC. Routing rules are defined as {group ID1, group ID2, Policy}. For example, as shown in Figure 4, the vRNIC of container 1 and container 2 are configured into the same group, thus allowing RDMA connections to be created. As a contrast, container 1 and VM 1 belong to different user groups and cannot establish an RDMA connection according to routing rules. See 4.3.1 for details on establishing RDMA connections between virtual instances.

% RDMA与传统TCP/IP不同，RDMA还支持单边操作。在guest应用的RDMA单边操作中，RDMA的工作请求中包含了远端的GVA及key，远端MR内存地址同样需要在RDMA用户态驱动中转换为远端vRNIC后端的HVA。为了便于远端地址的转换，我们在\sys core中建立了一个分布式的Key-Value数据库，存储各节点注册MR的映射关系{key，gva, hva}。在guest应用执行单边操作时，将向\sys core 中的分布式KV数据库查询，以获取远端MR的HVA，缓存到本地并在写入工作请求以前。

Unlike traditional TCP/IP network, RDMA networks also support one-side operations. The remote GVA and key is included in the one-side work request, and the GVA also needs to be translate to the remote HVA in the RDMA user-space. To facilitate the remote memory address translation, we set up a distributed key-value database in the \sys core to store the mapping of registered MR {MR-KEY, GVA, HVA} for each host. When the application in the guest performs a one-side operation, it will query the distributed key-value database in the \sys Core for the remote HVA of MR, and caches it locally before writing the work request.

% 总之，\sys core中的虚拟网络配置是动态调整的，因此，容器和虚拟机迁移后无需更改物理网卡及交换机等硬件的网络配置，实现便携的虚拟机或容器迁移。基于\sys core中的虚拟路由规则，可以支持云环境中的多租户隔离。而各\sys core存储的MR地址映射关系，对单边操作给予了支持。

In summary, the virtual network configuration in the \sys Core is dynamical, so it is unnecessary to change the network configuration of hardware such as the physical RNICs and switches to enable portable VM or container migration. Based on virtual routing rules in the \sys Core, multi-tenant isolation in a cloud environment can be supported. The MR memory mapping is recorded at each \sys core to support one-sided operations.

\begin{figure}[!ht]
	\centering
	\includegraphics[width=1.0\linewidth]{images/route-config}
	\caption{Group Configuration and Routing: The vRNICs of container 1 and container 2 are configured in one group. Thus, two containers can create RDMA connections. And VM 1 are not allowed to create RDMA connections to containers in this figure because it is not added into the group. }
	\label{fig:route-config}
\end{figure}

\subsection{Management Node}

There is one centralized management node in the cluster. With the help of management node, control policies can be defined and executed on each guest, e.g. QoS, ACL.

% 在\sys框架里，整个集群配备一个mgmt center，作为配置管理，存储各节点RDMA资源信息的中心。

In \sys, the cluster is equipped with one management center, which serves as configuration management and stores the information of RDMA resources for each node.

% 通过mgmt center，管理者向各个节点的\sys core下发各种策略，包括控制平面策略，如连接管理，防火墙等；和数据平面策略，如Qos、流量计费等。管理框架图如图5所示：策略由mgmt center下发到\sys core，\sys core通过agent接收策略后，其中，\sys core将控制平面策略配置给管理的各vRNIC后端。对于数据平面策略，\sys core将其发送给 Verbs库中的agent，然后在各虚拟机的RDMA应用本地执行。

Through the management center, various policies is issued to the \sys core for each node, including: control plane policies, such as connection management, firewall, etc; data plane policies, such as QoS, traffic metering, etc. The management framework is shown in Figure~\ref{fig:mgmt-center}: the policy is issued to the \sys Core by the management center and the \sys Core receives the policy through the agent. Then, the \sys Core configures the control plane policy to each managed vRNIC backend. For the data plane policy, the \sys Core sends it to the agent in the Verbs library, which is then executed locally in the application in each guest.

%  连接管理、防火墙等控制平面策略主要基于RDMA连接进行，依靠QP等RDMA资源。由于包括QP在内的所有RDMA资源均已经映射在vRNIC后端中，因此，控制平面策略可以直接在vRNIC后端依靠映射的RDMA资源执行；数据平面策略也可以依靠轮询各vRNIC后端的映射好的RDMA资源来完成。然而，这样做将消耗额外的CPU资源而且可能不正确如果轮询效率不高。因此，本文将数据策略进一步从\sys core下发到各guest的用户库，在各RDMA应用本地执行。注意，这些扩展需要所有客户机都信任库，并且这些修改应该包含在TCB(可信计算库)中。对照native的RDMA，其数据路径绕过了操作系统内核，物理网卡的数据管理及其有限，因此，其细粒度的数据管理同样需要在应用本地执行,并需要Verbs库的帮助。

The control plane policies are mainly based on RDMA connections, which rely on RDMA resources such as QP. Since all RDMA resources, including QP, are mapped to the vRNIC backend, the control plane policies can be executed in the vRNIC backend against the mapped RDMA resources. The data plane policies such as QoS also can be completed by polling the mapped RDMA resources in the vRNIC backend. However, the polling occupies additional CPU and may not be correct if polling is slow. Therefore, the data plane policies are further issued from \sys Core to each guest's user library, executed locally in each application. Note that these extensions require that the guest trusts its Verbs libraries, and that these changes should be included in the TCB(Trusted Computing Library). Compared with native RDMA, its data path bypasses the kernel, while the data management in physical RNIC is very limited. Therefore, the fine-grained data management of RDMA also needs to be executed locally in application with the help of the Verbs libraries.

\begin{figure}[!ht]
	\centering
	\includegraphics[width=1\linewidth]{images/mgmt-center.png}
	\caption{The Architecture of \sys Management Plane}
	\label{fig:mgmt-center}
\end{figure}

\subsection{Verbs Library and User-space Driver}

%% 两小节： 控制路径和数据路径
%% 给图
% Verbs库为RDMA应用提供了统一的Verbs接口，在控制路径上，Verbs库接受RDMA应用的Verbs命令并与RDMA内核驱动进行交互，在数据路径上，Verbs库调用设备相关用户态驱动，绕过了操作系统内核。为了让虚拟机或容器中的应用使用vRNIC，同时确保\sys对应用的透明性，Verbs库及其相关驱动需要适配vRNIC架构并提供与原生一致的Verbs接口。如图所示，针对控制路径和数据路径，在verbs库和用户驱动中有一些关键的修改工作。

The Verbs library provides the unified Verbs interface for RDMA applications. In the control path, the Verbs library forwards Verbs from RDMA applications to the RDMA kernel driver. In the data path, the Verbs library calls the device-specific user-space driver, which bypasses the kernel. To make \sys transparent to applications, the Verbs library and driver to be adapted to the vRNIC while keeping the same Verbs interface as native RDMA. As shown in Figure~\ref{fig:verbs-driver}, there are key modifications in the Verbs library and user-space driver for both control path and data path.

\begin{figure*}[!ht]
	\centering
	\includegraphics[width=0.8\linewidth]{images/verbs-driver}
	\caption{Major modifications in the Verbs library and User-space Driver for Containers and VMs}
	\label{fig:verbs-driver}
\end{figure*}

% 在容器中，为了让Verbs转发到vRNIC后端，将Verbs库中到内核驱动的接口替换为到vRNIC前端交互的接口，如图4-a-1所示。此外，为了让容器应用与vRNIC后端之间完成RDMA资源的共享内存映射，包括MR、QP/CQ和DoorBell，具体修改如下：
%（1）针对MR：在原生RDMA中，MR的buffer是由应用指定的，并且通常是私有内存。容器的Verbs用户库中将应用指定的MR内存重新映射到指定的共享内存，如图4-a-2所示。然后将该共享内存路径和GVA随Verbs命令及参数传递到vRNIC后端。vRNIC后端可以利用这一共享内存创建MR, 并将GVA传递到RDMA内核驱动，由RDMA内核驱动缓存给物理网卡。
%（2）针对QP/CQ：原生RDMA中QP/CQ内存的分配是在用户态驱动中进行的，也不是共享内存。在容器的用户态驱动中分配共享内存作为QP/CQ buffer，如图4-a-3所示。对应的共享内存信息会随Verbs命令和参数传递到vRNIC后端。
% vRNIC后端在创建QP/CQ时，将分配这一共享内存作为QP/CQ的buffer，如图4-a-4所示。

In the control path, there are different modifications between VMs and containers. In the container, to forward the Verbs calls to the vRNIC backend, the interface to the RDMA kernel driver in the Verbs library is replaced to the vRNIC frontend's, as shown in Figure~\ref{fig:verbs-driver} (a)-1. In addition, to achieve the shared memory mapping about RDMA resource between the application and the vRNIC backend, including MR, QP/CQ and Doorbell, there are more modifications in the Verbs library and user-space driver as follows:

(1) MR: In native RDMA, the MR buffer is assigned by the application and is generally private. The container's Verbs library will remap the GVA (guest virtual memory address) of the MR buffer to the shared physical memory, as shown in Figure~\ref{fig:verbs-driver} (a)-2. And the shared memory path and the GVA will be forwarded to the vRNIC backend along with the Verbs call and parameters. The vRNIC backend can get the shared memory to register the MR, forward the GVA to the RDMA kernel driver and there the GVA is cached for the physical RNIC.

(2) QP/CQ: In native RDMA, the QP/CQ buffer is allocated in user-space driver and not shared. In the container's user-space driver, the shared memory is allocated as QP/CQ buffer, as shown in Figure~\ref{fig:verbs-driver} (a)-3. The shared memory information will be also forwarded to the vRNIC backend along with the Verbs call and parameters. When the vRNIC backend creates QP/CQ, it will assign this shared memory as QP/CQ buffer, as shown in Figure~\ref{fig:verbs-driver} (a)-4.

%（3）针对DoorBell：在原生RDMA中，verbs库打开RDMA设备接口，经RDMA内核驱动映射门铃到应用程序。在\sys中，Verbs库打开虚拟的设备接口，并通过vRNIC后端和内核辅助模块来映射门铃。具体细节在第5节介绍。

(3) DoorBell: In native RDMA, the RDMA device is opened in the Verbs library and DoorBell is mapped to the applications through the RDMA kernel driver. In \sys, the container's Verbs library opens the virtual device interface and maps the DoorBell through the vRNIC backend and a costumed kernel module. The details will be introduced in Section 5.

% 在虚拟机中，vRNIC在实例化时已经共享了虚拟机的整块物理内存空间。即使虚拟机应用使用默认的方式分配MR、QP或CQ等RDMA资源的内存，无需修改，vRNIC后端可以获取对应的共享内存块。具体地，通过虚拟机内核的vRNIC前端驱动中，将这些buffer的GPA（虚拟机物理内存信息）随RDMA命令和参数转发给vRNIC后端，vRNIC后端可以将GPA翻译为HVA，并使用这些共享内存块作为RDMA资源MR，QP或CQ的buffer，如图4-b-4所示。

In the VM, the whole physical memory of the VM is already shared with the vRNIC backend when the vRNIC instantiating. Though the application allocates the buffers MR, QP/CQ without modification, the vRNIC backend still can get the corresponding shared memory piece. In specific, through the vRNIC frontend in the guest kernel, the GPA (guest physical memory addresses of these buffers can be forwarded to the vRNIC backend along with the Verbs calls and parameters, and the vRNIC backend can translate the GPA to the HVA (host virtual memory address) and use the shared memory pieces as the buffers of MR, QP/CQ resources, as shown in Figure~\ref{fig:verbs-driver} (b)-4.

% 基于上述修改，在数据路径上，无论是虚拟机还是容器，数据Verbs操作不需要转发到vRNIC后端,可以直接在guest用户空间，和原生RDMA一样避免了径额外的的拷贝或切换开销。

Based on the above modifications, for both VMs and containers,  the data Verbs operations are not forwarded to the vRNIC backend, and can executed directly in the guest user-space, which avoids the overhead of additional data copy or context switching as native RDMA.

\subsection{Virtual RDMA Workflow}

% 图4说明了虚拟RDMA的详细工作过程。虚拟RDMA的工作流程涉及guest中的应用、verbs库以及主机的vRNIC后端交互。整个工作流程依次分为初始化阶段、连接阶段和数据阶段。其中，初始化阶段分别包括设备打开和RDMA资源创建，连接阶段中RDMA应用通过QP与远端建立连接。数据阶段中RDMA应用执行数据路径的操作。

Figure~\ref{fig:rdma-path} illustrate the detailed workflow of virtual RDMA. The workflow of the virtual RDMA involves the application, the Verbs libraries (including the user-space driver) in the guest, and the vRNIC backend in the host. In sequence, the whole workflow can be divided into initialization phase, connection phase and data phase. The initialization phase includes device opening and RDMA resource initialization. In the connection phase, the RDMA application establishes a connection with the remote through QP. In the data phase, the RDMA application performs the operations in the data path.

\begin{figure}[!ht]
	\centering
	\includegraphics[width=1\linewidth]{images/RDMA-path.png}
	\caption{The Workflow of RDMA SEND operation}
	\label{fig:rdma-path}
\end{figure}

% 4.4.1 初始化阶段
%  初始化阶段主要完成资源的初始化，分别包括设备打开（I1）、注册内存buffer（I2）和创建QP/CQ等RDMA资源（I3）。具体细节如下： 
\subsubsection{\textbf{Initialization Phase}}
\
\noindent

Initialization stage is mainly about the initialization of resources, including vRNIC device opening (I1), MR registration (I2) and QP/CQ creation (I3). The details are as follows:

% I1（Guest应用打开vRNIC设备）：Guest应用依次调用Verbs接口ibv_get_devicelist和ibv_open_device，获取并打开vRNIC设备。这些调用会被劫持到vRNIC后端，vRNIC后端将打开在vRNIC实例化时绑定的物理RDMA设备。

I1(vRNIC Device Opening): The application gets and opens the vRNIC device by calling the Verbs ibv\_get\_device\_list and ibv\_open\_device in turn. The calls will be intercepted to the vRNIC backend, which will open the physical RNIC that was bound to the vRNIC when instantiation.

% I2（Guest应用注册MR）：在容器中，Verbs库中将MR buffer重新映射到某一共享内存，虚拟机无需此步骤， 因为当虚拟机启动时，整个虚拟机的内存已经被设置为共享的。vRNIC后端会获取对应的共享内存，并使用它来注册MR，并返回MR keys等信息。

I2(MR Registration): In the container, the Verbs library remaps the MR buffer to a shared memory. This step is not required in the VM, because the whole memory in the guest is already set to be shared when the guest boots. The vRNIC backend will obtain the corresponding shared memory, use it to register MR and return the MR keys. 

% I3（Guest应用创建QP/CQ）：在容器中，Verbs库使用共享内存分配QP/CQ buffer，虚拟机无需此步骤。vRNIC后端将获取对应的共享内存，并分配它为QP/CQ的buffer，并会返回QPN, CQN等元数据。 

I3(QP/CQ creation): In the container, the Verbs library use the shared memory to allocate QP/CQ buffer. The virtual machine does not need this step. The vRNIC backend will obtain the corresponding shared memory, allocate it for the QP/CQ buffer and return the metadata such as QPN, CQN.

%4.4.2 连接阶段
\subsubsection{\textbf{Connection Phase}}
\
\noindent
% 连接阶段表示RDMA基于QP建立连接，依次包括查询GID（C1）、交换连接信息（C2）和基于QP建立连接（C3），具体细节如下：

The connection phase means that the RDMA connection is established based on QP, including querying GID (C1), exchanging connection information (C2) and establishing connection (C3) in sequence. The details are as follows:

% C1（获取GID）：Guest应用调用Verbs ibv_query_gid，请求获取vRNIC的vGID地址。vRNIC后端将获取绑定的物理RDMA网卡的GID，记录（vGID，GID）对应关系, 并返回vGID给应用。 
C1 (Querying GID): The application calls Verbs ibv\_query\_gid for the vGID of vRNIC. The vRNIC backend will query the GID of the bounded physical RNIC, records the mapping {vGID, GID} and returns the vGID to the guest.

% C2（交换连接信息）：应用与远端交换建立连接所需的信息vGID，QPN和MR key等。 该步骤完全由应用独立完成。例如，应用中可以使用TCP/IP 或RDMA-CM来交换信息。
C2 (Exchanging Connection Information): The applications exchanges the connection information with the remote, such as vGID, QPN and MR-KEY. This step is done independently in the application. For example, the application can use TCP/IP or RDMA-CM to exchange the information.

% C3（连接建立）：应用调用verbs ibv_modify_qp 并将本地/远端的vGID、QPN等作为参数。该命令转发到vRNIC后端后，vRNIC后端会先根据\sys core的路由表规则进行判断，如果允许，就会将本地/远端 vGID转换为对应的GID，再根据QPN，在两个vRNIC后端的QP间建立RDMA连接。
C3 (Connection Establish): The application calls the Verb ibv\_modify\_qp with the parameters, including local/remote vGID, local/remote QPN. The vRNIC backend will check the routing table from \sys core. If allowed, the local/remote vGID will be converted to GID, and RDMA connection is established based on local/remote QP in two vRNIC backends through the QPN. 

% 4.4.3 数据阶段
% 数据路径发生在连接建立后，主要是对本地/远端 MR中的内容执行收发操作，包括图中的D1（收发数据）和D2 (轮询CQ）。具体如下：
\subsubsection{\textbf{Data Phase}}
\
\noindent

The data phase is after the RDMA connection is established and about how to send/receive the content in the local/remote MR buffers, including D1 (Send/Recv Data) and D2 (Poll the CQ). The details are as follows:

% D1（收发数据）：在双边操作中，两端应用分别执行Verbs ibv_post_recv和ibv_post_send对本地MR中的数据进行收发。在单边操作中，仅执行ibv_post_send。

D1 (Send/Recv Data): In two-side operation, data in the local MR can be sent and received by executing Verbs ibv\_post\_recv and ibv\_post\_send, respectively.In one-sided operations, only ibv\_post\_send is called.

% D2（轮询CQ）：Guest应用轮询CQ来获取对应的工作完成通知。如果使用事件通知机制，guest中verbs库将等待vRNIC后端获取事件并返回给应用。

D2 (Polling the CQ): The application poll the CQ for the work completions. If event-based notification is used, the Verbs libraries will wait for the vRNIC backend to receive the event and return it to the application.

% discussion
% 随着云计算环境的复杂化，不同的需求越来越多，对RDMA虚拟化提出了更多的要求。本节讨论了RDMA虚拟化面对下列需求可能的解决思路：
\subsection{Discussion}

In addition, we modified the Verbs library in the container so that the RDMA Verbs requests made to the kernel drivers can be redirected to the vRNIC Frontends. Note that Verbs APIs in the guest environment are not modified so that \sys is transparent to the guest RDMA applications. A few APIs in the Verbs library in the host user space are modified to support specifying a given memory address to be registered to the physical RNIC devices as key resource memory such as QP and MR. Currently, vanilla Verbs APIs don't allow applications to specify the address of resource memory region. But this capability is essential for \sys to establish a zero-copy data path in the virtual environment.


As cloud computing environments become more complex, there are more and more  requirements to RDMA virtualization. In this section, we discuss some possible solutions for following demands:

%（1）虚拟机迁移： 容器或虚拟机迁移在云中有很多好处，例如资源利用和故障转移。迁移分为脱机迁移和动态迁移。在脱机迁移中，虚拟机或容器关机后，迁移到远端后重新启动。因为\sys core维护了网络地址的映射关系，该关系可以直接发送给迁移后节点的\sys core，无需修改guest 中RDMA应用内部的网络地址。对于动态迁移，一般都需要借助AccelNet方案【引用】，应用释放RDMA资源，使用TCP/IP网络完成迁移，然后显式地重新建立RDMA连接。对于这种方式，我们也可以无缝支持。

Virtual Instances Migration:  Migration of containers or VMs has many benefits in clouds, e.g. resource utilization and fail-over. With the virtual RDMA network, \sys can support offline migration without reconfiguring the physical RDMA network for applications. In specific, after rebooting the migrated virtual instance, the application can rebuild the RDMA connection through the same network address. The only work is modifying the address mapping in \sys core. Currently, for live migrations, it is still hard because memory regions in RDMA application may be uncertain under bypassing or one-side communication. And the problem is unrelated to \sys.

% （2）RDMA软件栈对虚拟化的扩展：在RDMA虚拟化中，QP/CQ等RDMA资源需要进行映射，避免额外的数据拷贝及切换开销。现有的RDMA软件栈没有对RDMA资源映射的支持。为此，我们提出了下述建议，以加强现有RDMA软件栈对RDMA虚拟化的支持。基于这些建议，虚拟机或容器应用和虚拟层在用户空间便可以完成便捷、高效地完成MR、QP/CQ等资源的映射。具体建议如下：

Extension of RDMA software stack for virtualization: In RDMA virtualization, RDMA resources such as QP/CQ is mapped to avoid additional data copy and context switching. Existing RDMA software stack has no support for this mapping. Therefore, we propose the following suggestions to extend the existing RDMA software stack for RDMA virtualization. Based on them, the application in VMs or containers, and the virtualization layer can easily and efficiently map resources such as MR and QP/CQ in user-space. These suggestions are as follows:

% (1) 对于QP/CQ: 对于容器应用，在用户态驱动调用mlx4_alloc_buf分配QP/CQ的buffer时， 建议在其中增加使用共享内存的选择。对于虚拟层，在分配qp/cq资源buffer时，建议在用户态驱动调用新的API alloc_assigned_buf代替原有的mlx4_alloc_buf。这个新增的API，将为QP/CQ资源分配指定内存地址的buffer, 其中的地址参数由映射共享内存得到。

(1) For QP/CQ: In containers, when mlx4\_alloc\_buf is called to allocate QP/CQ buffer in user-space driver, it is recommended to add the option of using shared memory in mlx4\_alloc\_buf. In the virtualization layer, when creating QP/CQ, new API alloc\_assigned\_buf is suggested to be called in the user-space, instead of origin  mlx4\_alloc\_buf. This new API will allocates the buffer of the assigned memory address for QP/CQ, the parameter address is mapped from the shared memory.

% (2) 对于MR: 对于容器应用，我们建议应用使用共享内存注册MR。如果已有的应用未使用共享内存注册MR，在Verbs库ibv_reg_mr中，建议增加重新映射MR buffer到共享内存的选项。对于虚拟层，为了让虚拟机RDMA应用的数据路径与原生一致，需要将虚拟机MR的页表缓存到物理网卡中，以支持虚拟机RDMA应用的数据路径。为此，我们建议虚拟层Verbs中增加一个ibv_reg_guest_mr API. 该API比原生的注册MR函数ibv_reg_mr多一个参数，GVA，用以将GVA传递到RDMA内核OFED模块。同时，原生OFED模块也需进行适配性的改动，以通过GVA参数，将虚拟机MR的页表缓存到物理网卡。 经过以上扩展，数据路径上，虚拟机应用可以直接使用本地地址GVA，无需转换为虚拟层MR的地址HVA，也不用缓存本地/远端MR GVA和HVA之间的映射关系。

(2) For MR: For container applications, we recommend that applications register MR using shared memory. If an existing application does not use shared memory to register an MR, we  we recommend to add the option for remapping the MR buffer to shared memory in the Verbs library ibv\_reg\_mr. For the virtualization layer, to make the data path in the guest same as the native RDMA, the page table of the MR in the guest needs to be cached into the physical network card. For this reason, we propose to add an ibv\_reg\_guest\_mr API to the Verbs library in the virtualization layer. This new API has one more parameter "GVA" than the native registered MR API ibv\_reg\_mr, which passes the GVA to the RDMA kernel OFED module. At the same time, the native OFED module also needs to be adapted to cache the page table of the MR in the guest to the physical RNIC according to the GVA parameter. With this extension, the application in the guest can use the local address GVA directly in the data path, without converting it to the MR address in the virtualization layer (HVA), and without caching the mapping relationship between GVA and HVA about local/remote MR.