% 第4章 设计
\section{Design}

% URN的目标是实现一个统一的软件虚拟化架构，能够同时为虚拟机和容器提供RDMA服务，并实现对虚拟RDMA的统一管理。此外，该框架还需具备与原生RDMA接近的高性能。在本章，介绍了URN中的一些关键性设计工作。
The goal of URN is to provide a unified RDMA virtualization framework for the cloud environment where containers, virtual machines and containers inside virtual machine could coexist, even on the same server. In this framework, virtual and physical RDMA resources on one server are solely managed and scheduled by the URN core. The second design goal of URN is that the performance of virtualized RDMA should be close to the native RDMA. The third design goal is to be transparent to the vanilla RDMA applications, and to bring minimal code changes to RDMA software stack. In this section, we introduce some key design details.

\subsection{vRNIC Design}

% vRNIC是一个软件的虚拟RDMA设备，包含前后端，前端负责为上层verb库提供接口并转发RDMA 命令，后端在主机端，通过与URN core或verbs库交互，以模拟guest对vRNIC的RDMA操作。 我们继承了控制和数据通道分离的哲学，和已有的RDMA虚拟化方案一样，如hyv和MasQ，控制路径上guest应用在本地的虚假RDMA上下文中执行verbs命令，并经过FE转发到BE维持的真实RDMA上下文中执行，之后将结果返回给guest；在数据路径上，将guest应用的RDMA资源与vRNIC BE上下文中的RDMA资源进行映射，guest应用的RDMA资源被注册到了物理网卡中，之后guest应用可以在本地直接使用RDMA资源进行数据操作。
vRNIC is a virtual device, which consists of the frontend and backend. The frontend forward the commands of Verbs library in the guest user space to backend. The backend execute the commands by interacting with the physical RNIC through Verbs library in host user space. We inherited the philosophy that the control path and data path of RDMA operations are virtualized in different ways\cite{pfefferle2015hybrid}\cite{he2020masq}\cite{kim2019freeflow}. On the control path, para-virtualization technique is used that control commands from the guest Verb library are served by the frontend and forwarded to backend. On the data path, direct-passthrough technique is used that applications in the guest environment can directly map and use physical RDMA resources so that no guest OS kernel or host virtualization layer is involved when sending and receiving network pakages. The details of memory mapping implementations in URN framework is described in section 5 Implexxx.

% 前端：
% vRNIC前端作为vRNIC的设备接口提供给guest中的verbs库。Verbs命令可以通过前端转发到后端。在虚拟机中，vRNIC前端位于guest内核空间，并模拟原生的RDMA内核驱动为上层的verbs库提供与原生一致的接口，例如设备总线，设备文件。因此，当虚拟机中的前端加载后，可以直接使用未修改的verbs库。这样的好处是让URN系统更加稳定。
%The vRNIC frontend acts as the device interface of the vRNIC for Verbs library in the guest environments. The Verbs commands can be forwarded from the frontend to the backend. 
The design of the vRNIC frontend is different in containers and VMs. In VMs, the frontend is located in the guest kernel space, and it exposes the interface of native RDMA kernel driver to Verbs library, in the form of device class and device file. Thus, unmodified Verbs library can be used in VMs and this makes URN framework transparent to the evolution of Verbs library in the virtual machine.
% 然而，在容器中，无法像虚拟机一样提供对verbs用户库透明的接口。这是因为，如果前端位于主机内核空间，并提供与原生RDMA内核驱动一致的虚拟设备接口给verbs用户库。这些虚拟设备接口将与原生的物理设备接口一同暴露给verbs用户库. verbs用户库在未经修改的前提下， 无法区分使用的接口是前端提供的还是RDMA内核驱动提供的。
However,in containers, it is impossible to provide the unmodified interface for Verbs library. Because, if the frontend in host kernel space provides the same interfaces same as RDMA kernel driver. Note that both the virtual device interfaces and physical device interfaces are exposed to Verbs library in containers, and Verbs cannot distinguish them without extra modification in containers.  
% 因此，我们直接将前端实现在用户空间，作为一个用户库提供,并链接到Vers库。特别的，Verbs库中与设备接口相关的系统调用被替换为与前端交互的函数调用。容器应用程序在链接这个前端库后便可以转发verbs命令到后端。
%因此，容器前端的另一个好处是与Verbs交互完全在用户空间，没有系统调用开销。最后，注意虚拟机和容器的前端都是设备无关的，因为它们仅仅与通用的Verbs库及后端交互。
Thus, the frontend for containers is in user space, provided as a costumed library, and linked to Verbs library. In specific, in Verbs library, the origin device interface are replaced with the function call to the frontend. The Verbs commands can be forwarded to vRNIC backends after loading the frontend library. 
Thus, another advantage is that the interact with upper Verbs library is wholly finished in user space without the overhead of system calls. Finally, note that all frontends is hardware-independent because they only interact with the hardware-independent Verbs library and backends.


% 后端：后端在哪，为什么用户空间？
% 在URN中，vRNIC 后端实现在主机的用户空间作为一个虚拟设备。在用户空间主要有三方面的优势：1y用户空间更适合管理：开发管理功能，在用户空间更加简单并且灵活；2，用户空间的稳定性更好：由于用户空间不存在面向内核的攻击面；3，用户空间的兼容性更好，因为用户空间对于不同的系统架构、内核版本和设备内核驱动兼容.
 In URN, the vRNIC backend locates in the host user space as a virtual device. There are three advantages for vRNIC backend in user space: first, user space is more suitable for management: the develop of management functions is easier and more flexible in user space; second, user space provides higher stability: the attack surface for kernel is not in user-space; third, the user space provides higher compatibility because it is often independent on specific APIs (e.g. system architecture, kernel version and device kernel driver).

% 后端中需要维持哪些内容，为什么要维护？
% 后端作为虚拟设备，为guest提供的RDMA服务最终仍然需要由通过使用物理RNIC来完成。因此，在后端中，需要维护三方面的信息：1, 来自guest的虚拟的RDMA上下文信息： 该信息在接受前端请求时便可以记录下来，例如guest中应用的MR的虚拟地址GVA；2，主机中的真实RDMA上下文信息：这些信息直接通过verbs库来维持，并且能被物理网卡设备识别；3,两者的映射关系。在解析前端命令并调用Verbs库后，进行指定。基于上述信息，后端可以通过与UNR core进行交互，来实现对虚拟RDMA网络的管理以及对vRNIC使用硬件资源的统一管控。关于管理功能主要在URN core章节介绍。
As a virtual device, the RDMA service provided by the backend should be executed through the physical RNIC. Thus, there are three aspects of information need to be maintained: 
a) attributes for the virtual RDMA contexts in the guest environment: they can be recorded when the commands are received from the frontend in the guest, e.g. the GVA (guest virtual address) of MR in the application inside the guest.
b) attributes for the physical RDMA contexts in the host: they is maintained through the Verbs library and can be recognized by physical RNIC, e.g. the HVA (Host Virtual Memory) of MR in the host.
c) the mappings between virtual and physical attributes: the mapping relationship are assigned when the Verbs commands from the frontend are executed. 
Based on the above information, the vRNIC backend can interact with the URN core to achieve the virtual RDMA network management and the hardware source management. And the management are mainly introduced in section xxx.

\begin{figure}[!ht]
	\centering
	\includegraphics[width=0.7\linewidth]{images/vrnic-backend.png}
	\caption{The information maintained in vRNIC backends}
	\label{fig:route-config}
\end{figure}

\subsection{Verbs and Hardware-specific Libraries}
% 1，根据vRNIC的设计，需要对verbs库及设备相关库进行必要的修改。对于原生RDMA来说，verbs库是设备无关的，提供通用的verbs接口给上层应用，并通过通用的设备接口与内核驱动进行交互。硬件相关的库主要隐藏了与RDMA资源的重要细节，例如，资源的内存分配，以及QP队列的填充操作等等。
According the design of vRNIC, there are some necessary modifications in Verbs library and hardware-specific library, e.g. libmlx4. For native RDMA, the Verbs library is hardware-independent, which provides general interface to upper applications and interact with RDMA kernel driver through the general device interface. The hardware-specific library mainly hides the key details about manipulating RDMA resources, such as allocating memory and fillwithing QP. 

%在URN中，Verbs库中的修改主要与接口有关，在设备相关库中，修改主要与内存映射操作以及地址翻译等相关。因为内存映射在应用的内存地址和网卡记录的地址之间带来了不一致的问题。首先，在主机端，我们在设备相关库中，修改了地址分配的函数，在修改后，可以直接指定映射后的地址去注册RDMA资源到物理网卡。
In URN, the modifications in Verbs library is mainly about interfaces; And the modifications in hardware-specific library is mainly memory mapping operations, and the resource memory address translation, because the memory mapping brings inconsistent ion between the applications' memory address and the RNIC's recorded registered address (in vRNIC backend address space).

Firstly, in host, we modified the memory allocation about RDMA resources in hardware-specific library. After modification, we can assign the mapped memory when registering RDMA resources (e.g. QPs) to physical RNIC.

% 对于容器来说，我们修改了RDMA内核驱动与verbs之间的接口，并且用到前端的接口进行替换。对于容器来说，不同修改verbs用户库因为前端在内核中模拟了同样的设备接口。对于虚拟机和容器来说，所有的地址翻译都是需要的。
For containers, we modified interface between Verbs library and RDMA kernel driver, and replace with the interface to vRNIC frontend. For VMs, we do not modify the Verbs library due to the simulated device interface is same as native RDMA. For both VMs and containers, the address translation is needed in data path.

\subsection{URN Core}
% URN core被设计用来管理虚拟RDMA网络和物理RDMA资源在每个主机上， 包括实例化vRNIC后端，虚拟网络配置和RDMA资源限制等。同样的，URN实现在用户空间。
URN core is designed to manage the virtual and physical RDMA resources for the host, including instantiating vRNIC backends, virtual network configuration and RDMA resources limitation. Also, URN is in host user space for the same reasons in section xxx.

% URN core如何实例化vRNIC后端？
%注意到后端在主机用户空间，因此，URN core有必要建立一个消息通道在vRNIC和所有guest之间，因为URN core(主机用户空间) 是隔离虚拟机监视器或容器的。当guest需要vRNIC时，他们可以请求URN core在主机用户空间中实例化相应的后端。为了构建这个通道，我们使用了UNIX套接字对于虚拟机及容器环境。当虚拟机或容器需要一个新的vRNIC时，对套接字发出的实例化请求会被URN处理。为了快速支持多个实例化请求，这个过程中采用了反应器模式。
Recall that the vRNIC backends is in host user space, it is necessary to build a communication channel between URN and all guests, because the URN core is another process in host and the isolated with both containers and the hypervisor(VMs). When guests need a vRNIC, they can request the URN core to instantiate corresponding backend in host user space. To build this channel, we utilize the UNIX socket for both VMs and containers. When VMs or containers need a new vRNIC, the instantiating request to socket are deal with URN. To support multiple request rapidly, the reactor mode are adopted in the process. 


% URN core 对BE提供统一接口？
% 除了实例化vRNIC后端，URN core 还包括了管理功能:包括虚拟网络管理和硬件资源管理。 为了实现这些管理，URN core 首先要监控vRNIC后端的属性和上下文，并在URN核心中汇总信息(如资源内存映射)。为此，URN提供了记录RDMA上下文及属性的接口。在RNIC后端中，这些记录接口在Verbs命令执行后被调用，以更新记录信息;
%其次，URN core还提供了控制接口，并且用户定义的配置或控制策略可以部署到vRNIC后端。记录和控制接口都是回调函数，当vRNIC后端实例化时，它们被注册到vRNIC后端。接下来介绍虚拟网络配置和硬件资源管理:
Besides instantiating vRNIC backends, the URN core is also designed for the management: including virtual network and hardware resources. To achieve the management, at first, the attributes and contexts in vRNIC backends should be monitored and the information (e.g. resource memory mapping) should be summarized in URN core. Thus, the URN provides the recording interfaces about the attributes and contexts, and the interfaces are executed in vRNIC backends when the Verbs commands are executed; 
second, the control interfaces are also provided and user-defined configuration or control policies can be deployed into vRNIC backends. Both recording and control interfaces are lots of callback functions and they are registered into vRNIC backends when vRNIC backends instantiating. Later, the virtual network configurations an hardware resources management are introduced:

% 具体讲URN core提供了哪些管理功能：
% 1, 虚拟网络构建：在云环境中，虚拟网络配置是实现很多功能的基础，例如多租户隔离、便携的虚拟机或容器迁移。虚拟网络配置包括了虚拟的网络地址，虚拟的路由规则以及其他软件定义的配置协议。URN在用户空间，支持灵活的虚拟RDMA网络配置。
In clouds, virtual network configuration is important for lots of features, such as multi-tenant isolation and portable guest migrations. The virtual network configuration includes virtual network address, virtual network routing rules and other software-defined information. URN support flexible virtual RDMA network configurations in user space. 
%首先，通过vRNIC后端控制接口，URN可以用vRNIC后端维护的虚拟地址(例如vGID)配置vRNIC，该地址可以通过vRNIC前端反映到客户端的应用程序中。
First, through the control interface to vRNIC backends, URN can configure the vRNIC with virtual address (e.g. vGID) which maintained in vRNIC backends, and the address can be reflected into applications in the guest through the vRNIC frontend.
%其次，可以在每个URN核心中定义虚拟RDMA网络路由规则。每个vRNIC后端都配置了组ID，路由规则定义为:{group ID1, group ID2, Policy}。一个关于查找路由规则的回调函数在实例化时注册到每个vRNIC后端。这样，路由规则就可以实时地反映到服务器的所有vRNIC后端中。例如，如图~\ref{fig:route-config}所示，容器1和容器2的vrnic被配置到同一组中，因此允许创建RDMA连接。作为对比，容器和虚拟机1是隔离的因为属于不同组。最后，vgid在物理RNIC中不能被识别，网络地址映射在vRNIC后端被创建和更新。如果路由规则允许，这对vgid将被转换成物理的gid来创建RDMA连接.
Second, the virtual RDMA network routing rules can be defined in each URN core. Each vRNIC backend is configured with the group ID and the routing rules can be defined as: {Group ID1, Group ID2, Policy}. A callback function about lookuping routing rules is registered to each vRNIC backends when instantiating vRNIC backends. Thus, the routing rules can be reflected in real time for all vRNIC backends in the server. For example, as shown in Figure~\ref{fig:route-config}, the vRNICs for container 1 and container 2 are configured into the same group, thus RDMA connections are allowed to create. In contrast, the container 1 and VM 1 are isolated RDMA network because different groups. Finally, the vGIDs is not recognized in physical RNIC, and the network address mappings are created and updated in the vRNIC backends. If the routing rules allow, the pair of vGIDs will be translate to physical GIDs to create the RDMA connection.

\begin{figure}[!ht]
	\centering
	\includegraphics[width=1.0\linewidth]{images/route-config}
	\caption{Group Configuration and Routing: The vRNICs of container 1 and container 2 are configured in one group. Thus, two containers can create RDMA connections. And VM 1 are not allowed to create RDMA connections to containers in this figure because it is not added into the group. }
	\label{fig:route-config}
\end{figure}


\begin{figure}[!ht]
	\centering
	\includegraphics[width=0.7\linewidth]{images/urn-interface.png}
	\caption{URN Core Interface: QP examples}
	\label{fig:framework-overview}
\end{figure}

% 2 对硬件资源的管理： 主要分为两部分，一对vRNIC使用的资源进行限制，以QP资源为例子；二，对硬件资源通过灵活映射的方式提高利用率， 以VF映射为例子。
% 硬件资源管理是URN core的另一个重要特性。在URN内核中，每个vRNIC使用的硬件资源都是受限的。例如，QP是RDMA的关键资源，创建过多的QP可能会降低RDMA的性能。URN核心还可以通过记录接口监视服务器中的所有QPs。另外，可以在URN core中定义QPs的最大值，也可以将熔断策略注册到vRNIC后端。当QPs对于vRNIC过多时，后续的creat_qp将被中止。
Hardware resource management is another important feature in URN core. In URN core, hardware resources can be limited for each vRNIC. For example, QP is the key resource in RDMA and many QPs may decrease the performance of RDMA. The URN core can monitoring all QPs in the server through the recording interface. The maximum of QPs can be defined in URN core and the fusing policy can be also registered to vRNIC backends. When QPs are excessive for a vRNIC, the subsequent creat\_qp will be aborted. 
% 此外，URN核心中可以灵活地利用硬件资源。在URN core中，所有硬件资源的状态信息都可以通过各个vRNIC后端或Verbs库进行汇总。通过灵活地将虚拟RDMA上下文映射到物理硬件资源上，可以实现硬件资源的高效利用。例如，VFs可用于云中的性能隔离。但是VFs是有限的，有最大值，如63~126,而且VFs的分配是静态的。因此，如果客户机数量大于服务器中的VFs，则无法满足每个客户机的性能隔离。为了解决这个问题，URN core可以灵活地将VFs映射到vRNIC，实现每个客户端的性能隔离。当URN core发现vRNIC处于空闲状态时，URN core便会释放该vRNIC的VF， 并将其分配给其他已经就绪的vRNIC。
Besides, hardware resources can be utilized flexible in URN core. In URN core, the status of all hardware resources can be summarized by the vRNIC backends and the Verbs library. By flexibly mapping virtual RDMA contexts to physical hardware resources, efficient utilization of hardware resources can be achieved. For example, the VFs can be used for performance isolation in clouds. But VFs is limited with maximum e.g. 63~126, and the allocation of VFs is static. Thus, if the guests is more than the VFs in a server, the performance isolation for each guest is not meet. To solve this problem, the URN core can flexibly map VFs to vRNICs to achieve the performance isolation for each guest. If the URN core finds that the vRNIC is idle, the VF of the vRNIC is released by URN core and it can be allocated to another ready vRNIC.


\subsection{Discussions}
% 4.4 discussion 
% （1）云环境管理： 云环境中需要更多的管理功能，例如QoS，流量计费等策略。显然，这些策略依赖RDMA中数据路径的信息，例如消息大小。然而，在URN中，vRNIC前端和后端都在数据路径中被绕过。因此，我们应该将这些管理扩展到guest中的特定硬件库中，因为数据路径是在这个库中实现的。对于策略，可以通过管理中心进行配置和分发。注意，这些扩展需要所有客户机都信任库，并且这些修改应该包含在TCB(可信计算库)中。
(1) More management for clouds: There are lots of important managements in clouds, e.g. QoS, metering and so on. Apparently, these policies are dependent on the information about data path in RDMA, e.g. the message size. However, in URN, both vRNIC Frontend and Backend are bypassed in the data path. Thus, we should extend these management into specific-hardware library in guest, because data path is mainly implemented in this library. The policies can be defined and distribute through the management center. Note that, these extension needs that the library is trusted for all guests, and the modification should be included into TCB (trusted computing base).


% （2）虚拟实例迁移:容器或虚拟机迁移在云中有很多好处，例如资源利用和故障转移。通过使用虚拟RDMA网络，URN可以支持脱机迁移，而无需为应用程序重新配置物理RDMA网络。具体来说，在重新启动迁移的虚拟实例后，应用程序可以通过相同的网络地址重建RDMA连接。唯一的工作是修改URN core中的地址映射.目前，由于RDMA应用程序中的内存区域在旁路或单侧通信下可能存在不确定性，因此对动态迁移仍然存在困难。该问题与URN无关.
(2) Virtual Instances Migration: Migration of containers or VMs has many benefits in clouds, e.g. resource utilization and fail-over. With the virtual RDMA network, URN can support offline migration without reconfiguring the physical RDMA network for applications. In specific, after rebooting the migrated virtual instance, the application can rebuild the RDMA connection through the same network address. The only work is modifying the address mapping in URN core. Currently, for live migrations, it is still hard because memory regions in RDMA application may be uncertain under bypassing or one-side communication. And the problem is unrelated to URN.

% （3） 其他网络扩展： RDMA也可以被用来加速其他网络应用，例如基于TCP/IP网络协议栈的socket应用。现有的工作包括vSocket和SockDirect等。在URN中，同样可以通过扩展已有架构来实现优化guest socket的效果。通过修改vRNIC的前后端，可以将guest的socket命令转发到后端由RDMA执行。对应的，还需要修改与容器中的socket用户库，以将命令直接通过前端转发到后端。在后端，将socket命令通过RDMA网络执行，然后返回给guest。对于数据路径，同样可以通过映射方式实现零拷贝。
(3) Other network extensions: RDMA can also be exploited to optimize the performance of other network applications,  such as TCP/IP.  Existing works include vSocket ~\cite{wang2019vsocket}, SockDirect~\cite{li2019socksdirect} so on. In URN, the socket applications in the guest can also be optimized through some extensions. By modifying the vRNIC frontend and backends, the socket commands can be forwarded to the backend and executed through physical RDMA network. For data paths, zero copy can also be achieved by mapping.
